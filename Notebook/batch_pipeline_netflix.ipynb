{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaf70d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_pipeline_netflix.ipynb\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, count\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. Spark session with both GCS + BigQuery connectors\n",
    "# -------------------------------------------------------------------\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"NetflixBatchPipeline\")\n",
    "        .master(\"spark://spark-master:7077\")\n",
    "        .config(\"spark.driver.memory\", \"4g\")\n",
    "        .config(\"spark.executor.memory\", \"4g\")\n",
    "        # Explicitly include the GCS connector we just installed\n",
    "        .config(\"spark.jars\", \"/usr/local/spark/jars/gcs-connector-hadoop3-latest.jar\")\n",
    "        # Enable authentication with your GCP key\n",
    "        .config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\")\n",
    "        .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\",\n",
    "                \"/home/jovyan/work/notebooks/keys/service-account.json\")\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. Read Netflix data from your GCS bucket\n",
    "# -------------------------------------------------------------------\n",
    "netflix_bucket = \"gs://netflix_data_25/\"\n",
    "users_df  = spark.read.option(\"header\", True).csv(f\"{netflix_bucket}users.csv\")\n",
    "movies_df = spark.read.option(\"header\", True).csv(f\"{netflix_bucket}movies.csv\")\n",
    "watch_df  = spark.read.option(\"header\", True).csv(f\"{netflix_bucket}watch_history.csv\")\n",
    "\n",
    "print(f\"Users: {users_df.count()}  Movies: {movies_df.count()}  Watch: {watch_df.count()}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3. Join + aggregate (example transformation)\n",
    "# -------------------------------------------------------------------\n",
    "joined_df = (\n",
    "    watch_df.join(users_df, \"user_id\", \"left\")\n",
    "             .join(movies_df, \"movie_id\", \"left\")\n",
    ")\n",
    "\n",
    "agg_df = (\n",
    "    joined_df.groupBy(\"country\", \"subscription_plan\")\n",
    "             .agg(\n",
    "                 count(\"*\").alias(\"total_sessions\"),\n",
    "                 avg(col(\"progress_percentage\")).alias(\"avg_progress\")\n",
    "             )\n",
    ")\n",
    "\n",
    "agg_df.show(10, truncate=False)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4. Write result to BigQuery\n",
    "# -------------------------------------------------------------------\n",
    "(\n",
    "    agg_df.write.format(\"bigquery\")\n",
    "        .option(\"table\", \"de2025-471807.netflix.country_engagement_summary\")\n",
    "        .option(\"temporaryGcsBucket\", \"netflix-group5-temp\")   # temp bucket you create once\n",
    "        .mode(\"overwrite\")\n",
    "        .save()\n",
    ")\n",
    "\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de-data-architecture-pipelines",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
