{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Netflix Real-Time Streaming Pipeline\n",
        "\n",
        "This notebook implements a Spark Structured Streaming pipeline to analyze Netflix viewing events in real-time.\n",
        "\n",
        "## Architecture\n",
        "- **Data Source**: watch_history.csv with synthetic timestamps\n",
        "- **Streaming Engine**: Spark Structured Streaming (file-based)\n",
        "- **Analytics**: Windowed aggregations (5-min and 10-min windows)\n",
        "- **Output**: In-memory tables queryable with SQL\n",
        "\n",
        "## Cells Overview\n",
        "1. **Data Preparation**: Load watch_history, add timestamps, save as JSON files\n",
        "2. **Spark Setup**: Configure SparkSession\n",
        "3. **Schema & Static Data**: Define schema, load movies catalog\n",
        "4. **Trending Content**: 5-minute windowed aggregations\n",
        "5. **User Engagement**: 10-minute windowed aggregations\n",
        "6. **Action Monitoring**: Track user actions (play, pause, stop, complete)\n",
        "7. **Live Display**: Query and display results every 10 seconds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Data Preparation - Add Timestamps and Create JSON Files for Streaming\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"NETFLIX STREAMING PIPELINE - DATA PREPARATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Configuration\n",
        "GCS_BUCKET = \"gs://data_netflix_2025/raw\"\n",
        "LOCAL_DATA_PATH = \"/home/jovyan/data\"\n",
        "WATCH_HISTORY_FILE = f\"{GCS_BUCKET}/watch_history.csv\"\n",
        "OUTPUT_STREAM_DIR = f\"{LOCAL_DATA_PATH}/netflix_stream\"\n",
        "EVENTS_PER_FILE = 150  # Number of events per JSON file\n",
        "\n",
        "print(f\"\\nüì• Loading watch_history from: {WATCH_HISTORY_FILE}\")\n",
        "\n",
        "# Load watch history data\n",
        "df = pd.read_csv(WATCH_HISTORY_FILE)\n",
        "print(f\"‚úÖ Loaded {len(df)} watch events\")\n",
        "print(f\"   Columns: {list(df.columns)}\")\n",
        "\n",
        "# Display sample of original data\n",
        "print(\"\\nüìä Sample of original data (first 5 rows):\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\n‚è∞ Adding realistic timestamps to watch_date...\")\n",
        "\n",
        "# Function to generate realistic hour distribution (peak evening hours)\n",
        "def generate_realistic_hour():\n",
        "    \"\"\"Generate hour with peak probability in evening (18:00-23:00)\"\"\"\n",
        "    # Define hour probabilities (higher for evening)\n",
        "    hours = list(range(24))\n",
        "    probabilities = [\n",
        "        0.01, 0.01, 0.01, 0.01, 0.01, 0.02,  # 0-5 (early morning, low)\n",
        "        0.02, 0.03, 0.03, 0.03, 0.03, 0.04,  # 6-11 (morning, low-medium)\n",
        "        0.04, 0.04, 0.04, 0.04, 0.05, 0.05,  # 12-17 (afternoon, medium)\n",
        "        0.08, 0.09, 0.10, 0.10, 0.08, 0.06   # 18-23 (evening, peak)\n",
        "    ]\n",
        "    return np.random.choice(hours, p=probabilities)\n",
        "\n",
        "# Add timestamp components\n",
        "np.random.seed(42)  # For reproducibility\n",
        "df['hour'] = [generate_realistic_hour() for _ in range(len(df))]\n",
        "df['minute'] = np.random.randint(0, 60, len(df))\n",
        "df['second'] = np.random.randint(0, 60, len(df))\n",
        "\n",
        "# Create full timestamp\n",
        "df['timestamp'] = pd.to_datetime(df['watch_date']) + \\\n",
        "                  pd.to_timedelta(df['hour'], unit='h') + \\\n",
        "                  pd.to_timedelta(df['minute'], unit='m') + \\\n",
        "                  pd.to_timedelta(df['second'], unit='s')\n",
        "\n",
        "# Sort by timestamp to create proper event stream\n",
        "df = df.sort_values('timestamp').reset_index(drop=True)\n",
        "\n",
        "print(f\"‚úÖ Added timestamps. Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
        "\n",
        "# Drop temporary columns, keep original watch_date for reference\n",
        "df = df.drop(['hour', 'minute', 'second'], axis=1)\n",
        "\n",
        "# Convert timestamp to string format for JSON\n",
        "df['timestamp'] = df['timestamp'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "print(\"\\nüìä Sample data with timestamps:\")\n",
        "print(df[['session_id', 'user_id', 'movie_id', 'timestamp', 'action']].head(10))\n",
        "\n",
        "# Create output directory (remove if exists to start fresh)\n",
        "if os.path.exists(OUTPUT_STREAM_DIR):\n",
        "    print(f\"\\nüóëÔ∏è  Removing existing directory: {OUTPUT_STREAM_DIR}\")\n",
        "    shutil.rmtree(OUTPUT_STREAM_DIR)\n",
        "\n",
        "os.makedirs(OUTPUT_STREAM_DIR, exist_ok=True)\n",
        "print(f\"‚úÖ Created output directory: {OUTPUT_STREAM_DIR}\")\n",
        "\n",
        "# Split data into multiple JSON files for streaming simulation\n",
        "print(f\"\\nüìù Creating JSON files ({EVENTS_PER_FILE} events per file)...\")\n",
        "\n",
        "num_files = 0\n",
        "for i in range(0, len(df), EVENTS_PER_FILE):\n",
        "    batch = df.iloc[i:i+EVENTS_PER_FILE]\n",
        "    \n",
        "    # Convert batch to JSON records\n",
        "    records = batch.to_dict(orient='records')\n",
        "    \n",
        "    # Write to JSON file\n",
        "    file_path = f\"{OUTPUT_STREAM_DIR}/part-{num_files:05d}.json\"\n",
        "    with open(file_path, 'w') as f:\n",
        "        for record in records:\n",
        "            f.write(json.dumps(record) + '\\n')\n",
        "    \n",
        "    num_files += 1\n",
        "    \n",
        "    if num_files % 100 == 0:\n",
        "        print(f\"   Created {num_files} files...\")\n",
        "\n",
        "print(f\"\\n‚úÖ Created {num_files} JSON files in {OUTPUT_STREAM_DIR}\")\n",
        "print(f\"   Total events: {len(df)}\")\n",
        "print(f\"   Average events per file: {len(df) / num_files:.1f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ DATA PREPARATION COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nStreaming source directory: {OUTPUT_STREAM_DIR}\")\n",
        "print(f\"Ready for Spark Structured Streaming with maxFilesPerTrigger=1\")\n",
        "print(\"\\nNext: Run Cell 2 to configure Spark and start streaming queries.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Spark Session Configuration\n",
        "\n",
        "from pyspark import SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SPARK SESSION CONFIGURATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Configure Spark\n",
        "sparkConf = SparkConf()\n",
        "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
        "sparkConf.setAppName(\"NetflixStreamingPipeline\")\n",
        "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
        "sparkConf.set(\"spark.executor.memory\", \"2g\")\n",
        "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
        "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
        "\n",
        "print(\"\\n‚öôÔ∏è  Spark Configuration:\")\n",
        "print(f\"   Master: spark://spark-master:7077\")\n",
        "print(f\"   App Name: NetflixStreamingPipeline\")\n",
        "print(f\"   Driver Memory: 2g\")\n",
        "print(f\"   Executor Memory: 2g\")\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
        "\n",
        "print(\"\\n‚úÖ Spark session created successfully!\")\n",
        "print(f\"   Spark Version: {spark.version}\")\n",
        "print(f\"   Session ID: {spark.sparkContext.applicationId}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Ready to define schemas and load data!\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Schema Definition & Load Static Data\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, BooleanType, IntegerType, TimestampType\n",
        "from pyspark.sql.functions import col, to_timestamp\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SCHEMA DEFINITION & STATIC DATA LOADING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Define schema for watch history streaming data\n",
        "print(\"\\nüìã Defining schema for watch history events...\")\n",
        "\n",
        "dataSchema = StructType([\n",
        "    StructField(\"session_id\", StringType(), True),\n",
        "    StructField(\"user_id\", StringType(), True),\n",
        "    StructField(\"movie_id\", StringType(), True),\n",
        "    StructField(\"watch_date\", StringType(), True),\n",
        "    StructField(\"device_type\", StringType(), True),\n",
        "    StructField(\"watch_duration_minutes\", DoubleType(), True),\n",
        "    StructField(\"progress_percentage\", DoubleType(), True),\n",
        "    StructField(\"action\", StringType(), True),\n",
        "    StructField(\"quality\", StringType(), True),\n",
        "    StructField(\"location_country\", StringType(), True),\n",
        "    StructField(\"is_download\", BooleanType(), True),\n",
        "    StructField(\"user_rating\", DoubleType(), True),\n",
        "    StructField(\"timestamp\", StringType(), True)  # Will be converted to TimestampType\n",
        "])\n",
        "\n",
        "print(\"‚úÖ Schema defined with 13 fields\")\n",
        "print(\"   Key fields: session_id, user_id, movie_id, timestamp, action\")\n",
        "\n",
        "# Load movies catalog as static DataFrame for enrichment\n",
        "print(\"\\nüì• Loading movies catalog (static data)...\")\n",
        "\n",
        "GCS_BUCKET = \"gs://data_netflix_2025/raw\"\n",
        "MOVIES_FILE = f\"{GCS_BUCKET}/movies.csv\"\n",
        "\n",
        "movies_static = spark.read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .csv(MOVIES_FILE)\n",
        "\n",
        "movies_count = movies_static.count()\n",
        "print(f\"‚úÖ Loaded {movies_count} movies from catalog\")\n",
        "\n",
        "# Select relevant columns for enrichment\n",
        "movies_static = movies_static.select(\n",
        "    \"movie_id\",\n",
        "    \"title\",\n",
        "    \"content_type\",\n",
        "    \"genre_primary\",\n",
        "    \"genre_secondary\",\n",
        "    \"release_year\",\n",
        "    \"duration_minutes\",\n",
        "    \"rating\",\n",
        "    \"imdb_rating\"\n",
        ")\n",
        "\n",
        "print(\"\\nüìä Movies catalog schema:\")\n",
        "movies_static.printSchema()\n",
        "\n",
        "print(\"\\nüìä Sample movies:\")\n",
        "movies_static.show(5, truncate=False)\n",
        "\n",
        "# Cache the movies dataframe for better performance in joins\n",
        "movies_static.cache()\n",
        "print(\"\\n‚úÖ Movies catalog cached for streaming joins\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Schema and static data ready!\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nNext: Create streaming queries with windowed aggregations\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Streaming Query - Trending Content (5-minute Windows)\n",
        "\n",
        "from pyspark.sql.functions import window, count, avg, sum as spark_sum, desc\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"STREAMING QUERY 1: TRENDING CONTENT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Configuration\n",
        "STREAM_INPUT_DIR = \"/home/jovyan/data/netflix_stream\"\n",
        "MAX_FILES_PER_TRIGGER = 1\n",
        "\n",
        "print(f\"\\nüì° Setting up streaming source...\")\n",
        "print(f\"   Input directory: {STREAM_INPUT_DIR}\")\n",
        "print(f\"   Max files per trigger: {MAX_FILES_PER_TRIGGER}\")\n",
        "\n",
        "# Read streaming data\n",
        "sdf = spark.readStream \\\n",
        "    .schema(dataSchema) \\\n",
        "    .option(\"maxFilesPerTrigger\", MAX_FILES_PER_TRIGGER) \\\n",
        "    .json(STREAM_INPUT_DIR)\n",
        "\n",
        "print(\"‚úÖ Streaming source configured\")\n",
        "\n",
        "# Convert timestamp string to TimestampType\n",
        "sdf = sdf.withColumn(\"timestamp\", to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd HH:MM:ss\"))\n",
        "\n",
        "print(\"\\nüîó Joining streaming data with movies catalog...\")\n",
        "\n",
        "# Join with static movies data for enrichment\n",
        "enriched = sdf.join(movies_static, \"movie_id\", \"left\")\n",
        "\n",
        "print(\"‚úÖ Stream enriched with movie metadata\")\n",
        "\n",
        "# Create windowed aggregation for trending content\n",
        "print(\"\\nüìä Creating 5-minute windowed aggregation for trending content...\")\n",
        "\n",
        "trending = enriched \\\n",
        "    .groupBy(\n",
        "        window(col(\"timestamp\"), \"5 minutes\"),\n",
        "        \"movie_id\",\n",
        "        \"title\",\n",
        "        \"genre_primary\"\n",
        "    ) \\\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"view_count\"),\n",
        "        avg(\"progress_percentage\").alias(\"avg_completion\"),\n",
        "        count(col(\"user_id\")).alias(\"total_sessions\")\n",
        "    ) \\\n",
        "    .select(\n",
        "        col(\"window.start\").alias(\"window_start\"),\n",
        "        col(\"window.end\").alias(\"window_end\"),\n",
        "        \"movie_id\",\n",
        "        \"title\",\n",
        "        \"genre_primary\",\n",
        "        \"view_count\",\n",
        "        \"avg_completion\",\n",
        "        \"total_sessions\"\n",
        "    ) \\\n",
        "    .orderBy(col(\"view_count\").desc())\n",
        "\n",
        "print(\"‚úÖ Trending content aggregation defined\")\n",
        "\n",
        "# Write stream to memory sink for querying\n",
        "print(\"\\nüöÄ Starting streaming query (trending_content)...\")\n",
        "\n",
        "query_trending = trending \\\n",
        "    .writeStream \\\n",
        "    .queryName(\"trending_content\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .start()\n",
        "\n",
        "print(\"‚úÖ Streaming query 'trending_content' started successfully!\")\n",
        "print(f\"   Query ID: {query_trending.id}\")\n",
        "print(f\"   Status: {query_trending.status}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Trending content query running!\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nAnalytics:\")\n",
        "print(\"  - 5-minute tumbling windows\")\n",
        "print(\"  - Top movies by view count\")\n",
        "print(\"  - Average completion percentage\")\n",
        "print(\"  - Total viewing sessions per movie\")\n",
        "print(\"\\nQuery results available in 'trending_content' table\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Streaming Query - User Engagement (10-minute Windows)\n",
        "\n",
        "from pyspark.sql.functions import countDistinct\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"STREAMING QUERY 2: USER ENGAGEMENT\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nüìä Creating 10-minute windowed aggregation for user engagement...\")\n",
        "\n",
        "# User engagement metrics: active users, watch time, session duration\n",
        "engagement = enriched \\\n",
        "    .groupBy(\n",
        "        window(col(\"timestamp\"), \"10 minutes\"),\n",
        "        \"device_type\",\n",
        "        \"location_country\"\n",
        "    ) \\\n",
        "    .agg(\n",
        "        countDistinct(\"user_id\").alias(\"active_users\"),\n",
        "        spark_sum(\"watch_duration_minutes\").alias(\"total_watch_time\"),\n",
        "        avg(\"watch_duration_minutes\").alias(\"avg_session_duration\"),\n",
        "        count(\"*\").alias(\"total_sessions\")\n",
        "    ) \\\n",
        "    .select(\n",
        "        col(\"window.start\").alias(\"window_start\"),\n",
        "        col(\"window.end\").alias(\"window_end\"),\n",
        "        \"device_type\",\n",
        "        \"location_country\",\n",
        "        \"active_users\",\n",
        "        \"total_watch_time\",\n",
        "        \"avg_session_duration\",\n",
        "        \"total_sessions\"\n",
        "    ) \\\n",
        "    .orderBy(col(\"active_users\").desc())\n",
        "\n",
        "print(\"‚úÖ User engagement aggregation defined\")\n",
        "\n",
        "# Write stream to memory sink\n",
        "print(\"\\nüöÄ Starting streaming query (user_engagement)...\")\n",
        "\n",
        "query_engagement = engagement \\\n",
        "    .writeStream \\\n",
        "    .queryName(\"user_engagement\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .start()\n",
        "\n",
        "print(\"‚úÖ Streaming query 'user_engagement' started successfully!\")\n",
        "print(f\"   Query ID: {query_engagement.id}\")\n",
        "print(f\"   Status: {query_engagement.status}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"User engagement query running!\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nAnalytics:\")\n",
        "print(\"  - 10-minute tumbling windows\")\n",
        "print(\"  - Active users by device and country\")\n",
        "print(\"  - Total watch time per segment\")\n",
        "print(\"  - Average session duration\")\n",
        "print(\"  - Total viewing sessions\")\n",
        "print(\"\\nQuery results available in 'user_engagement' table\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Streaming Query - Action Monitoring\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"STREAMING QUERY 3: ACTION MONITORING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nüìä Creating 5-minute windowed aggregation for action monitoring...\")\n",
        "\n",
        "# Action monitoring: track started, paused, stopped, completed events\n",
        "actions = enriched \\\n",
        "    .groupBy(\n",
        "        window(col(\"timestamp\"), \"5 minutes\"),\n",
        "        \"action\"\n",
        "    ) \\\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"action_count\"),\n",
        "        countDistinct(\"user_id\").alias(\"unique_users\"),\n",
        "        avg(\"progress_percentage\").alias(\"avg_progress\")\n",
        "    ) \\\n",
        "    .select(\n",
        "        col(\"window.start\").alias(\"window_start\"),\n",
        "        col(\"window.end\").alias(\"window_end\"),\n",
        "        \"action\",\n",
        "        \"action_count\",\n",
        "        \"unique_users\",\n",
        "        \"avg_progress\"\n",
        "    ) \\\n",
        "    .orderBy(col(\"action_count\").desc())\n",
        "\n",
        "print(\"‚úÖ Action monitoring aggregation defined\")\n",
        "\n",
        "# Write stream to memory sink\n",
        "print(\"\\nüöÄ Starting streaming query (action_monitoring)...\")\n",
        "\n",
        "query_actions = actions \\\n",
        "    .writeStream \\\n",
        "    .queryName(\"action_monitoring\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .start()\n",
        "\n",
        "print(\"‚úÖ Streaming query 'action_monitoring' started successfully!\")\n",
        "print(f\"   Query ID: {query_actions.id}\")\n",
        "print(f\"   Status: {query_actions.status}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Action monitoring query running!\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nAnalytics:\")\n",
        "print(\"  - 5-minute tumbling windows\")\n",
        "print(\"  - Count of each action type (started, paused, stopped, completed)\")\n",
        "print(\"  - Unique users performing each action\")\n",
        "print(\"  - Average progress percentage per action\")\n",
        "print(\"\\nQuery results available in 'action_monitoring' table\")\n",
        "\n",
        "# Additional: Genre Performance Query\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STREAMING QUERY 4: GENRE PERFORMANCE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nüìä Creating 5-minute windowed aggregation for genre performance...\")\n",
        "\n",
        "genre_performance = enriched \\\n",
        "    .groupBy(\n",
        "        window(col(\"timestamp\"), \"5 minutes\"),\n",
        "        \"genre_primary\"\n",
        "    ) \\\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"view_count\"),\n",
        "        countDistinct(\"user_id\").alias(\"unique_viewers\"),\n",
        "        avg(\"progress_percentage\").alias(\"avg_completion\"),\n",
        "        spark_sum(\"watch_duration_minutes\").alias(\"total_watch_time\")\n",
        "    ) \\\n",
        "    .select(\n",
        "        col(\"window.start\").alias(\"window_start\"),\n",
        "        col(\"window.end\").alias(\"window_end\"),\n",
        "        \"genre_primary\",\n",
        "        \"view_count\",\n",
        "        \"unique_viewers\",\n",
        "        \"avg_completion\",\n",
        "        \"total_watch_time\"\n",
        "    ) \\\n",
        "    .orderBy(col(\"view_count\").desc())\n",
        "\n",
        "print(\"‚úÖ Genre performance aggregation defined\")\n",
        "\n",
        "print(\"\\nüöÄ Starting streaming query (genre_performance)...\")\n",
        "\n",
        "query_genre = genre_performance \\\n",
        "    .writeStream \\\n",
        "    .queryName(\"genre_performance\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .start()\n",
        "\n",
        "print(\"‚úÖ Streaming query 'genre_performance' started successfully!\")\n",
        "print(f\"   Query ID: {query_genre.id}\")\n",
        "print(f\"   Status: {query_genre.status}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"All streaming queries running!\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nActive Queries:\")\n",
        "print(\"  1. trending_content (5-min windows)\")\n",
        "print(\"  2. user_engagement (10-min windows)\")\n",
        "print(\"  3. action_monitoring (5-min windows)\")\n",
        "print(\"  4. genre_performance (5-min windows)\")\n",
        "print(\"\\nNext: Run Cell 7 to display live results\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Display Real-Time Results\n",
        "\n",
        "from time import sleep\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"REAL-TIME STREAMING DASHBOARD\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nStarting live display loop...\")\n",
        "print(\"Press Ctrl+C (or interrupt kernel) to stop\")\n",
        "print(\"\\nUpdates every 10 seconds\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "try:\n",
        "    for i in range(50):  # Run for ~8 minutes (50 iterations * 10 seconds)\n",
        "        print(\"\\n\\n\")\n",
        "        print(\"‚ñà\" * 80)\n",
        "        print(f\"UPDATE #{i+1} - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "        print(\"‚ñà\" * 80)\n",
        "        \n",
        "        # 1. Trending Content\n",
        "        print(\"\\nüìä TOP 10 TRENDING CONTENT (Last 5 minutes):\")\n",
        "        print(\"-\" * 80)\n",
        "        trending_df = spark.sql(\"\"\"\n",
        "            SELECT \n",
        "                window_start,\n",
        "                window_end,\n",
        "                movie_id,\n",
        "                title,\n",
        "                genre_primary,\n",
        "                view_count,\n",
        "                ROUND(avg_completion, 2) as avg_completion_pct,\n",
        "                total_sessions\n",
        "            FROM trending_content\n",
        "            ORDER BY view_count DESC\n",
        "            LIMIT 10\n",
        "        \"\"\")\n",
        "        trending_df.show(10, truncate=False)\n",
        "        \n",
        "        # 2. User Engagement\n",
        "        print(\"\\nüë• USER ENGAGEMENT BY DEVICE & COUNTRY (Last 10 minutes):\")\n",
        "        print(\"-\" * 80)\n",
        "        engagement_df = spark.sql(\"\"\"\n",
        "            SELECT \n",
        "                window_start,\n",
        "                window_end,\n",
        "                device_type,\n",
        "                location_country,\n",
        "                active_users,\n",
        "                ROUND(total_watch_time, 2) as total_watch_time_min,\n",
        "                ROUND(avg_session_duration, 2) as avg_session_min,\n",
        "                total_sessions\n",
        "            FROM user_engagement\n",
        "            ORDER BY active_users DESC\n",
        "            LIMIT 10\n",
        "        \"\"\")\n",
        "        engagement_df.show(10, truncate=False)\n",
        "        \n",
        "        # 3. Action Monitoring\n",
        "        print(\"\\nüì± ACTION MONITORING (Last 5 minutes):\")\n",
        "        print(\"-\" * 80)\n",
        "        actions_df = spark.sql(\"\"\"\n",
        "            SELECT \n",
        "                window_start,\n",
        "                window_end,\n",
        "                action,\n",
        "                action_count,\n",
        "                unique_users,\n",
        "                ROUND(avg_progress, 2) as avg_progress_pct\n",
        "            FROM action_monitoring\n",
        "            ORDER BY action_count DESC\n",
        "        \"\"\")\n",
        "        actions_df.show(truncate=False)\n",
        "        \n",
        "        # 4. Genre Performance\n",
        "        print(\"\\nüé¨ TOP GENRES (Last 5 minutes):\")\n",
        "        print(\"-\" * 80)\n",
        "        genre_df = spark.sql(\"\"\"\n",
        "            SELECT \n",
        "                window_start,\n",
        "                window_end,\n",
        "                genre_primary,\n",
        "                view_count,\n",
        "                unique_viewers,\n",
        "                ROUND(avg_completion, 2) as avg_completion_pct,\n",
        "                ROUND(total_watch_time, 2) as total_watch_time_min\n",
        "            FROM genre_performance\n",
        "            ORDER BY view_count DESC\n",
        "            LIMIT 10\n",
        "        \"\"\")\n",
        "        genre_df.show(10, truncate=False)\n",
        "        \n",
        "        # Query Status\n",
        "        print(\"\\nüîÑ STREAMING QUERY STATUS:\")\n",
        "        print(\"-\" * 80)\n",
        "        print(f\"  trending_content:   {query_trending.status['isDataAvailable']} | Recent progress: {query_trending.recentProgress[-1]['numInputRows'] if query_trending.recentProgress else 0} rows\")\n",
        "        print(f\"  user_engagement:    {query_engagement.status['isDataAvailable']} | Recent progress: {query_engagement.recentProgress[-1]['numInputRows'] if query_engagement.recentProgress else 0} rows\")\n",
        "        print(f\"  action_monitoring:  {query_actions.status['isDataAvailable']} | Recent progress: {query_actions.recentProgress[-1]['numInputRows'] if query_actions.recentProgress else 0} rows\")\n",
        "        print(f\"  genre_performance:  {query_genre.status['isDataAvailable']} | Recent progress: {query_genre.recentProgress[-1]['numInputRows'] if query_genre.recentProgress else 0} rows\")\n",
        "        \n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(f\"Next update in 10 seconds... (Iteration {i+1}/50)\")\n",
        "        print(\"=\" * 80)\n",
        "        \n",
        "        sleep(10)\n",
        "        \n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n\\n\" + \"üõë\" * 40)\n",
        "    print(\"STOPPING STREAMING PIPELINE\")\n",
        "    print(\"üõë\" * 40)\n",
        "    print(\"\\nStopping all queries gracefully...\")\n",
        "    \n",
        "    query_trending.stop()\n",
        "    print(\"‚úÖ Stopped: trending_content\")\n",
        "    \n",
        "    query_engagement.stop()\n",
        "    print(\"‚úÖ Stopped: user_engagement\")\n",
        "    \n",
        "    query_actions.stop()\n",
        "    print(\"‚úÖ Stopped: action_monitoring\")\n",
        "    \n",
        "    query_genre.stop()\n",
        "    print(\"‚úÖ Stopped: genre_performance\")\n",
        "    \n",
        "    print(\"\\n‚úÖ All streaming queries stopped successfully!\")\n",
        "    print(\"\\nNote: Spark session is still active. Run the next cell to stop it completely.\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error occurred: {str(e)}\")\n",
        "    print(\"\\nStopping queries...\")\n",
        "    \n",
        "    try:\n",
        "        query_trending.stop()\n",
        "        query_engagement.stop()\n",
        "        query_actions.stop()\n",
        "        query_genre.stop()\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    print(\"Queries stopped due to error\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Display loop completed or interrupted\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: Stop Spark Session (Run after stopping queries)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"CLEANUP: STOPPING SPARK SESSION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Stop Spark context\n",
        "spark.stop()\n",
        "\n",
        "print(\"\\n‚úÖ Spark session stopped successfully!\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Netflix Streaming Pipeline Complete!\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nSummary:\")\n",
        "print(\"  - Data preparation: ‚úÖ\")\n",
        "print(\"  - Streaming queries: ‚úÖ\")\n",
        "print(\"  - Real-time analytics: ‚úÖ\")\n",
        "print(\"  - Cleanup: ‚úÖ\")\n",
        "print(\"\\nThank you for using the Netflix Streaming Pipeline!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
