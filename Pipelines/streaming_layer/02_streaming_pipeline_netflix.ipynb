{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ddb1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# CELL 1: Variables Configuration\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# GCP Settings\n",
    "GCP_PROJECT_ID = \"agile-producer-471907-s7\"\n",
    "GCS_BUCKET = \"data_netflix_2025\"\n",
    "TEMP_GCS_BUCKET = \"temp_netflix_2025\"\n",
    "\n",
    "# Data Paths\n",
    "STREAMING_DATA_PATH = f\"gs://{GCS_BUCKET}/streaming\"\n",
    "USERS_CATALOG_PATH = f\"gs://{GCS_BUCKET}/raw/users.csv\"\n",
    "MOVIES_CATALOG_PATH = f\"gs://{GCS_BUCKET}/raw/movies.csv\"\n",
    "\n",
    "# BigQuery Settings\n",
    "BQ_DATASET = \"netflix_streaming\"\n",
    "BQ_OUTPUT_TABLE = \"engagement_health_realtime\" \n",
    "BQ_TABLE_FULL = f\"{GCP_PROJECT_ID}.{BQ_DATASET}.{BQ_OUTPUT_TABLE}\"\n",
    "\n",
    "# Spark Settings\n",
    "SPARK_MASTER = \"spark://spark-master:7077\"\n",
    "DRIVER_MEMORY = \"2g\"\n",
    "EXECUTOR_MEMORY = \"2g\"\n",
    "EXECUTOR_CORES = \"1\"\n",
    "\n",
    "# Streaming Settings\n",
    "MAX_FILES_PER_TRIGGER = 1\n",
    "WINDOW_DURATION = \"5 minutes\"\n",
    "TRIGGER_INTERVAL = \"10 seconds\"\n",
    "WATERMARK_DELAY = \"10 minutes\"\n",
    "\n",
    "# Display Settings\n",
    "DISPLAY_ITERATIONS = 30\n",
    "DISPLAY_INTERVAL_SECONDS = 10\n",
    "TOP_N_RESULTS = 10\n",
    "\n",
    "LOW_ENGAGEMENT_THRESHOLD = 20.0 \n",
    "\n",
    "print(\"Configuration loaded successfully\")\n",
    "print(f\"Output destination: {BQ_TABLE_FULL}\")\n",
    "print(f\"Streaming source: {STREAMING_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374671c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# CELL 2: SPARK SESSION SETUP\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"INITIALIZING SPARK SESSION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Configure Spark\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(SPARK_MASTER)\n",
    "sparkConf.setAppName(\"NetflixStreamingPipeline\")\n",
    "sparkConf.set(\"spark.driver.memory\", DRIVER_MEMORY)\n",
    "sparkConf.set(\"spark.executor.memory\", EXECUTOR_MEMORY)\n",
    "sparkConf.set(\"spark.executor.cores\", EXECUTOR_CORES)\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "# Configure GCS access\n",
    "spark.conf.set('temporaryGcsBucket', TEMP_GCS_BUCKET)\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "print(f\"Spark session created: {spark.version}\")\n",
    "print(f\"Application ID: {spark.sparkContext.applicationId}\")\n",
    "print(\"GCS access configured\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb6c7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# CELL 3: Static Data Loading\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, BooleanType\n",
    "from pyspark.sql.functions import col, when, coalesce, lit\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LOADING & CLEANING STATIC DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Watch events schema (same as before)\n",
    "watch_events_schema = StructType([\n",
    "    StructField(\"session_id\", StringType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"movie_id\", StringType(), True),\n",
    "    StructField(\"watch_date\", StringType(), True),\n",
    "    StructField(\"device_type\", StringType(), True),\n",
    "    StructField(\"watch_duration_minutes\", DoubleType(), True),\n",
    "    StructField(\"progress_percentage\", DoubleType(), True),\n",
    "    StructField(\"action\", StringType(), True),\n",
    "    StructField(\"quality\", StringType(), True),\n",
    "    StructField(\"location_country\", StringType(), True),\n",
    "    StructField(\"is_download\", BooleanType(), True),\n",
    "    StructField(\"user_rating\", DoubleType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Load USERS catalog\n",
    "print(f\"\\n1. Loading users from: {USERS_CATALOG_PATH}\")\n",
    "users_static = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(USERS_CATALOG_PATH)\n",
    "\n",
    "# Clean users data\n",
    "users_static = users_static.select(\n",
    "    \"user_id\",\n",
    "    \"country\",\n",
    "    \"subscription_plan\",\n",
    "    coalesce(col(\"monthly_spend\"), lit(0.0)).alias(\"monthly_spend\"),  # Handle nulls\n",
    "    coalesce(col(\"age\"), lit(0)).cast(\"int\").alias(\"age\"),\n",
    "    coalesce(col(\"gender\"), lit(\"Unknown\")).alias(\"gender\"),\n",
    "    \"is_active\"\n",
    ").cache()\n",
    "\n",
    "print(f\"Users loaded: {users_static.count():,}\")\n",
    "print(\"Sample users:\")\n",
    "users_static.show(3, truncate=False)\n",
    "\n",
    "# Load MOVIES catalog\n",
    "print(f\"\\n2. Loading movies from: {MOVIES_CATALOG_PATH}\")\n",
    "movies_static = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(MOVIES_CATALOG_PATH)\n",
    "\n",
    "# Clean movies data\n",
    "movies_static = movies_static.select(\n",
    "    \"movie_id\",\n",
    "    \"title\",\n",
    "    \"content_type\",\n",
    "    \"genre_primary\",\n",
    "    coalesce(col(\"imdb_rating\"), lit(0.0)).alias(\"imdb_rating\"),  # Handle nulls\n",
    "    coalesce(col(\"is_netflix_original\"), lit(False)).alias(\"is_netflix_original\"),\n",
    "    \"release_year\",\n",
    "    \"rating\"\n",
    ").cache()\n",
    "\n",
    "print(f\"Movies loaded: {movies_static.count():,}\")\n",
    "print(\"Sample movies:\")\n",
    "movies_static.show(3, truncate=False)\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3496ff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# CELL 4: Streaming Query with Churn Detection\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    window, col, count, avg, sum as spark_sum,\n",
    "    approx_count_distinct, to_timestamp, desc,\n",
    "    when, isnan,lit, round as spark_round, max as spark_max\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BUILDING ENGAGEMENT & CHURN RISK PIPELINE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Read streaming data\n",
    "print(f\"Setting up streaming source: {STREAMING_DATA_PATH}\")\n",
    "stream_df = spark.readStream \\\n",
    "    .schema(watch_events_schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", MAX_FILES_PER_TRIGGER) \\\n",
    "    .json(STREAMING_DATA_PATH)\n",
    "\n",
    "# Convert timestamp and clean nulls\n",
    "stream_df = stream_df \\\n",
    "    .withColumn(\"event_time\", to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "    .withColumn(\"progress_percentage\", \n",
    "        when(isnan(col(\"progress_percentage\")) | col(\"progress_percentage\").isNull(), lit(0.0))\n",
    "        .otherwise(col(\"progress_percentage\"))\n",
    "    ) \\\n",
    "    .withColumn(\"watch_duration_minutes\",\n",
    "        when(isnan(col(\"watch_duration_minutes\")) | col(\"watch_duration_minutes\").isNull(), lit(0.0))\n",
    "        .otherwise(col(\"watch_duration_minutes\"))\n",
    "    )\n",
    "    \n",
    "# JOIN 1: Enrich with USER data\n",
    "enriched_stream = stream_df.join(users_static, \"user_id\", \"left\")\n",
    "print(\"âœ“ Joined with users (country, subscription, monthly_spend)\")\n",
    "\n",
    "# JOIN 2: Enrich with MOVIE data\n",
    "enriched_stream = enriched_stream.join(movies_static, \"movie_id\", \"left\")\n",
    "print(\"âœ“ Joined with movies (genre, imdb_rating, is_netflix_original)\")\n",
    "\n",
    "# Add CHURN SIGNAL FLAGS\n",
    "enriched_stream = enriched_stream \\\n",
    "    .withColumn(\"is_low_engagement\", \n",
    "                when(col(\"progress_percentage\") < LOW_ENGAGEMENT_THRESHOLD, 1).otherwise(0)) \\\n",
    "    .withColumn(\"is_quality_downgrade\",\n",
    "                when((col(\"quality\") == \"SD\") | (col(\"quality\") == \"HD\"), 1).otherwise(0)) \\\n",
    "    .withColumn(\"is_abandoned\",\n",
    "                when((col(\"action\") == \"stopped\") & (col(\"progress_percentage\") < 50), 1).otherwise(0)) \\\n",
    "    .withColumn(\"is_completed\",\n",
    "                when(col(\"action\") == \"completed\", 1).otherwise(0))\n",
    "\n",
    "print(\"âœ“ Churn signal flags added (low_engagement, quality_downgrade, abandoned, completed)\")\n",
    "\n",
    "# Add watermark\n",
    "enriched_stream = enriched_stream.withWatermark(\"event_time\", WATERMARK_DELAY)\n",
    "print(f\"âœ“ Watermark configured: {WATERMARK_DELAY}\")\n",
    "\n",
    "# AGGREGATION: Multi-dimensional engagement health\n",
    "print(f\"\\nBuilding aggregation: {WINDOW_DURATION} windows\")\n",
    "engagement_health = enriched_stream \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), WINDOW_DURATION),\n",
    "        \"country\",\n",
    "        \"genre_primary\",\n",
    "        \"device_type\",\n",
    "        \"quality\",\n",
    "        \"is_netflix_original\",\n",
    "        \"content_type\"\n",
    "    ) \\\n",
    "    .agg(\n",
    "        # Core engagement metrics\n",
    "        count(\"*\").alias(\"total_sessions\"),\n",
    "        approx_count_distinct(\"user_id\").alias(\"unique_users\"),\n",
    "        spark_sum(\"watch_duration_minutes\").alias(\"total_watch_minutes\"),\n",
    "        avg(\"progress_percentage\").alias(\"avg_completion_pct\"),\n",
    "        \n",
    "        # Content quality signals\n",
    "        avg(\"imdb_rating\").alias(\"avg_content_quality\"),\n",
    "        \n",
    "        # Churn risk signals\n",
    "        spark_sum(\"is_low_engagement\").alias(\"low_engagement_count\"),\n",
    "        spark_sum(\"is_abandoned\").alias(\"abandonment_count\"),\n",
    "        spark_sum(\"is_quality_downgrade\").alias(\"quality_downgrade_count\"),\n",
    "        spark_sum(\"is_completed\").alias(\"completion_count\"),\n",
    "        \n",
    "        # Business metrics\n",
    "        spark_sum(\"monthly_spend\").alias(\"total_revenue_impact\"),\n",
    "        avg(\"monthly_spend\").alias(\"avg_subscriber_value\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        # Window\n",
    "        col(\"window.start\").alias(\"window_start\"),\n",
    "        col(\"window.end\").alias(\"window_end\"),\n",
    "        \n",
    "        # Dimensions\n",
    "        \"country\",\n",
    "        \"genre_primary\",\n",
    "        \"device_type\",\n",
    "        \"quality\",\n",
    "        \"is_netflix_original\",\n",
    "        \"content_type\",\n",
    "        \n",
    "        # Core metrics\n",
    "        \"total_sessions\",\n",
    "        \"unique_users\",\n",
    "        spark_round(\"total_watch_minutes\", 1).alias(\"total_watch_minutes\"),\n",
    "        spark_round(\"avg_completion_pct\", 2).alias(\"avg_completion_pct\"),\n",
    "        spark_round(\"avg_content_quality\", 2).alias(\"avg_content_quality\"),\n",
    "        \n",
    "        # Churn signals\n",
    "        \"low_engagement_count\",\n",
    "        \"abandonment_count\",\n",
    "        \"quality_downgrade_count\",\n",
    "        \"completion_count\",\n",
    "        \n",
    "        # Calculated KPIs\n",
    "        spark_round((col(\"low_engagement_count\") / col(\"total_sessions\") * 100), 2).alias(\"low_engagement_rate\"),\n",
    "        spark_round((col(\"abandonment_count\") / col(\"total_sessions\") * 100), 2).alias(\"abandonment_rate\"),\n",
    "        spark_round((col(\"completion_count\") / col(\"total_sessions\") * 100), 2).alias(\"completion_rate\"),\n",
    "        \n",
    "        # Business metrics\n",
    "        spark_round(\"total_revenue_impact\", 2).alias(\"revenue_impact\"),\n",
    "        spark_round(\"avg_subscriber_value\", 2).alias(\"avg_subscriber_value\"),\n",
    "        \n",
    "        # Alert level (HIGH if >60% low engagement)\n",
    "        when(col(\"low_engagement_count\") / col(\"total_sessions\") > 0.6, \"HIGH\")\n",
    "            .when(col(\"low_engagement_count\") / col(\"total_sessions\") > 0.4, \"MEDIUM\")\n",
    "            .otherwise(\"LOW\").alias(\"alert_level\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"low_engagement_rate\"))  # Order by worst engagement first\n",
    "\n",
    "print(\"âœ“ Aggregation complete: engagement health with churn signals\")\n",
    "print(\"\\nMetrics included:\")\n",
    "print(\"  - Engagement: sessions, users, watch time, completion rate\")\n",
    "print(\"  - Churn Signals: low engagement rate, abandonment rate\")\n",
    "print(\"  - Quality: content rating, quality downgrades\")\n",
    "print(\"  - Business: revenue impact, subscriber value\")\n",
    "print(\"  - Alerts: automated alert levels (HIGH/MEDIUM/LOW)\")\n",
    "\n",
    "# Define BigQuery write function\n",
    "def write_to_bigquery(batch_df, batch_id):\n",
    "    \"\"\"Write engagement health metrics to BigQuery\"\"\"\n",
    "    print(f\"Writing batch {batch_id} to BigQuery...\")\n",
    "    if batch_df.count() > 0:\n",
    "        batch_df.write.format('bigquery') \\\n",
    "            .option('table', BQ_TABLE_FULL) \\\n",
    "            .mode(\"append\")  \\\n",
    "            .save()\n",
    "        print(f\"âœ“ Batch {batch_id} written ({batch_df.count()} rows)\")\n",
    "    else:\n",
    "        print(f\"âŠ˜ Batch {batch_id} is empty\")\n",
    "\n",
    "# Start streaming queries\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STARTING STREAMING QUERIES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Memory sink (for monitoring)\n",
    "query_memory = engagement_health \\\n",
    "    .writeStream \\\n",
    "    .queryName(\"engagement_health_memory\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .start()\n",
    "\n",
    "print(f\"âœ“ Memory sink started: {query_memory.id}\")\n",
    "\n",
    "# BigQuery sink\n",
    "query_bigquery = engagement_health \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .trigger(processingTime=TRIGGER_INTERVAL) \\\n",
    "    .foreachBatch(write_to_bigquery) \\\n",
    "    .start()\n",
    "\n",
    "print(f\"âœ“ BigQuery sink started: {query_bigquery.id}\")\n",
    "print(f\"âœ“ Output table: {BQ_TABLE_FULL}\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89de0f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# CELL 5: Live Monitoring Display\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ENGAGEMENT & CHURN RISK DASHBOARD\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Update interval: {DISPLAY_INTERVAL_SECONDS} seconds\")\n",
    "print(f\"Showing HIGH RISK segments (sorted by low_engagement_rate)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    for iteration in range(DISPLAY_ITERATIONS):\n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(f\"UPDATE #{iteration + 1} | {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"{'=' * 80}\")\n",
    "        \n",
    "        # Query top risk segments\n",
    "        result_df = spark.sql(f\"\"\"\n",
    "            SELECT \n",
    "                window_start,\n",
    "                country,\n",
    "                genre_primary,\n",
    "                device_type,\n",
    "                quality,\n",
    "                total_sessions,\n",
    "                unique_users,\n",
    "                avg_completion_pct,\n",
    "                low_engagement_rate,\n",
    "                abandonment_rate,\n",
    "                alert_level\n",
    "            FROM engagement_health_memory\n",
    "            WHERE total_sessions >= 1\n",
    "            ORDER BY low_engagement_rate DESC, total_sessions DESC\n",
    "            LIMIT {TOP_N_RESULTS}\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"\\nðŸš¨ HIGHEST CHURN RISK SEGMENTS:\")\n",
    "        result_df.show(TOP_N_RESULTS, truncate=False)\n",
    "        \n",
    "        # Summary stats\n",
    "        summary = spark.sql(\"\"\"\n",
    "            SELECT \n",
    "                COUNT(*) as total_segments,\n",
    "                SUM(total_sessions) as total_sessions,\n",
    "                AVG(low_engagement_rate) as avg_churn_risk,\n",
    "                SUM(CASE WHEN alert_level = 'HIGH' THEN 1 ELSE 0 END) as high_risk_segments\n",
    "            FROM engagement_health_memory\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"\\nðŸ“Š OVERALL HEALTH:\")\n",
    "        summary.show(truncate=False)\n",
    "        \n",
    "        print(f\"\\nQuery Status: Memory={query_memory.isActive}, BigQuery={query_bigquery.isActive}\")\n",
    "        \n",
    "        sleep(DISPLAY_INTERVAL_SECONDS)\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nðŸ›‘ Stopping pipelines...\")\n",
    "    query_memory.stop()\n",
    "    query_bigquery.stop()\n",
    "    print(\"âœ“ Stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787b32f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# CELL 6: CLEANUP - STOP SPARK SESSION\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CLEANUP: STOPPING SPARK SESSION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "spark.stop()\n",
    "\n",
    "print(\"Spark session stopped successfully\")\n",
    "print(\"\\nPipeline Summary:\")\n",
    "print(f\"  - Processed streaming events from: {STREAMING_DATA_PATH}\")\n",
    "print(f\"  - Output written to: {BQ_TABLE_FULL}\")\n",
    "print(f\"  - Ready for Looker Studio dashboard creation\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
