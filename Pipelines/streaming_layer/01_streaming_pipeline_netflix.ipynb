{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d243bfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NETFLIX STREAMING PIPELINE - REAL-TIME TRENDING CONTENT ANALYSIS\n",
    "# ============================================================================\n",
    "# This notebook processes Netflix viewing events in real-time and tracks\n",
    "# trending content using 5-minute windows. Results are written to BigQuery\n",
    "# for visualization in Looker Studio.\n",
    "#\n",
    "# Based on: Lab9 patterns (file streaming, windowed aggregations, BigQuery sink)\n",
    "# ============================================================================\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION SECTION\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# GCP Settings\n",
    "GCP_PROJECT_ID = \"agile-producer-471907-s7\"\n",
    "GCS_BUCKET = \"data_netflix_2025\"\n",
    "TEMP_GCS_BUCKET = \"temp_netflix_2025\"\n",
    "\n",
    "# Data Paths\n",
    "STREAMING_DATA_PATH = f\"gs://{GCS_BUCKET}/streaming\"\n",
    "MOVIES_CATALOG_PATH = f\"gs://{GCS_BUCKET}/raw/movies.csv\"\n",
    "\n",
    "# BigQuery Settings\n",
    "BQ_DATASET = \"netflix_streaming\"\n",
    "BQ_OUTPUT_TABLE = \"trending_content_realtime\"\n",
    "BQ_TABLE_FULL = f\"{GCP_PROJECT_ID}.{BQ_DATASET}.{BQ_OUTPUT_TABLE}\"\n",
    "\n",
    "# Spark Settings\n",
    "SPARK_MASTER = \"spark://spark-master:7077\"\n",
    "DRIVER_MEMORY = \"2g\"\n",
    "EXECUTOR_MEMORY = \"2g\"\n",
    "EXECUTOR_CORES = \"1\"\n",
    "\n",
    "# Streaming Settings\n",
    "MAX_FILES_PER_TRIGGER = 1\n",
    "WINDOW_DURATION = \"5 minutes\"\n",
    "TRIGGER_INTERVAL = \"10 seconds\"\n",
    "WATERMARK_DELAY = \"10 minutes\"\n",
    "\n",
    "# Display Settings\n",
    "DISPLAY_ITERATIONS = 30\n",
    "DISPLAY_INTERVAL_SECONDS = 10\n",
    "TOP_N_RESULTS = 10\n",
    "\n",
    "print(\"Configuration loaded successfully\")\n",
    "print(f\"Output destination: {BQ_TABLE_FULL}\")\n",
    "print(f\"Streaming source: {STREAMING_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671bbe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# CELL 1: SPARK SESSION SETUP\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"INITIALIZING SPARK SESSION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Configure Spark\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(SPARK_MASTER)\n",
    "sparkConf.setAppName(\"NetflixStreamingPipeline\")\n",
    "sparkConf.set(\"spark.driver.memory\", DRIVER_MEMORY)\n",
    "sparkConf.set(\"spark.executor.memory\", EXECUTOR_MEMORY)\n",
    "sparkConf.set(\"spark.executor.cores\", EXECUTOR_CORES)\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "# Configure GCS access\n",
    "spark.conf.set('temporaryGcsBucket', TEMP_GCS_BUCKET)\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "print(f\"Spark session created: {spark.version}\")\n",
    "print(f\"Application ID: {spark.sparkContext.applicationId}\")\n",
    "print(\"GCS access configured\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586edc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# CELL 2: SCHEMA DEFINITION & STATIC DATA\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, BooleanType\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SCHEMA DEFINITION & STATIC DATA LOADING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define schema for streaming watch events\n",
    "watch_events_schema = StructType([\n",
    "    StructField(\"session_id\", StringType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"movie_id\", StringType(), True),\n",
    "    StructField(\"watch_date\", StringType(), True),\n",
    "    StructField(\"device_type\", StringType(), True),\n",
    "    StructField(\"watch_duration_minutes\", DoubleType(), True),\n",
    "    StructField(\"progress_percentage\", DoubleType(), True),\n",
    "    StructField(\"action\", StringType(), True),\n",
    "    StructField(\"quality\", StringType(), True),\n",
    "    StructField(\"location_country\", StringType(), True),\n",
    "    StructField(\"is_download\", BooleanType(), True),\n",
    "    StructField(\"user_rating\", DoubleType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "print(\"Watch events schema defined (13 fields)\")\n",
    "\n",
    "# Load movies catalog as static DataFrame\n",
    "print(f\"\\nLoading movies catalog from: {MOVIES_CATALOG_PATH}\")\n",
    "movies_static = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(MOVIES_CATALOG_PATH)\n",
    "\n",
    "movies_count = movies_static.count()\n",
    "print(f\"Loaded {movies_count} movies\")\n",
    "\n",
    "# Select and cache relevant movie attributes\n",
    "movies_static = movies_static.select(\n",
    "    \"movie_id\",\n",
    "    \"title\",\n",
    "    \"content_type\",\n",
    "    \"genre_primary\",\n",
    "    \"release_year\"\n",
    ").cache()\n",
    "\n",
    "print(\"Movies catalog cached for streaming joins\")\n",
    "print(\"Sample movies:\")\n",
    "movies_static.show(3, truncate=False)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2248289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# CELL 3: STREAMING QUERY - TRENDING CONTENT WITH BIGQUERY OUTPUT\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    window, col, count, avg, sum as spark_sum,\n",
    "    approx_count_distinct, to_timestamp, desc\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CONFIGURING STREAMING QUERY: TRENDING CONTENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Read streaming data from GCS\n",
    "print(f\"Setting up streaming source: {STREAMING_DATA_PATH}\")\n",
    "stream_df = spark.readStream \\\n",
    "    .schema(watch_events_schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", MAX_FILES_PER_TRIGGER) \\\n",
    "    .json(STREAMING_DATA_PATH)\n",
    "\n",
    "print(f\"Streaming source configured (maxFilesPerTrigger={MAX_FILES_PER_TRIGGER})\")\n",
    "\n",
    "# Convert timestamp string to TimestampType\n",
    "stream_df = stream_df.withColumn(\n",
    "    \"event_time\",\n",
    "    to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\")\n",
    ")\n",
    "\n",
    "# Join with static movies catalog\n",
    "enriched_stream = stream_df.join(movies_static, \"movie_id\", \"left\")\n",
    "print(\"Stream enriched with movies catalog (static join)\")\n",
    "\n",
    "# Add watermark for handling late data\n",
    "enriched_stream = enriched_stream.withWatermark(\"event_time\", WATERMARK_DELAY)\n",
    "print(f\"Watermark configured: {WATERMARK_DELAY}\")\n",
    "\n",
    "# Create windowed aggregation: Trending content\n",
    "print(f\"\\nBuilding aggregation: {WINDOW_DURATION} windows\")\n",
    "trending_content = enriched_stream \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), WINDOW_DURATION),\n",
    "        \"movie_id\",\n",
    "        \"title\",\n",
    "        \"genre_primary\",\n",
    "        \"content_type\"\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"view_count\"),\n",
    "        approx_count_distinct(\"user_id\").alias(\"unique_viewers\"),\n",
    "        avg(\"progress_percentage\").alias(\"avg_completion_pct\"),\n",
    "        spark_sum(\"watch_duration_minutes\").alias(\"total_watch_time_min\"),\n",
    "        count(col(\"action\") == \"completed\").alias(\"completed_views\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"window.start\").alias(\"window_start\"),\n",
    "        col(\"window.end\").alias(\"window_end\"),\n",
    "        \"movie_id\",\n",
    "        \"title\",\n",
    "        \"genre_primary\",\n",
    "        \"content_type\",\n",
    "        \"view_count\",\n",
    "        \"unique_viewers\",\n",
    "        \"avg_completion_pct\",\n",
    "        \"total_watch_time_min\",\n",
    "        \"completed_views\"\n",
    "    ) \\\n",
    "    .orderBy(desc(\"view_count\"))\n",
    "\n",
    "print(\"Aggregation built: view_count, unique_viewers, avg_completion, total_watch_time\")\n",
    "\n",
    "# Define BigQuery write function\n",
    "def write_to_bigquery(batch_df, batch_id):\n",
    "    \"\"\"\n",
    "    Write each micro-batch to BigQuery.\n",
    "    This function is called for each streaming batch.\n",
    "    \"\"\"\n",
    "    print(f\"Writing batch {batch_id} to BigQuery...\")\n",
    "    if batch_df.count() > 0:\n",
    "        batch_df.write.format('bigquery') \\\n",
    "            .option('table', BQ_TABLE_FULL) \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save()\n",
    "        print(f\"Batch {batch_id} written successfully ({batch_df.count()} rows)\")\n",
    "    else:\n",
    "        print(f\"Batch {batch_id} is empty, skipping write\")\n",
    "\n",
    "# Start streaming query with memory sink (for monitoring)\n",
    "print(\"\\nStarting streaming query: Memory sink (for live display)\")\n",
    "query_memory = trending_content \\\n",
    "    .writeStream \\\n",
    "    .queryName(\"trending_content_memory\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .start()\n",
    "\n",
    "print(f\"Memory sink started: {query_memory.id}\")\n",
    "\n",
    "# Start streaming query with BigQuery sink (for Looker Studio)\n",
    "print(\"\\nStarting streaming query: BigQuery sink (for dashboard)\")\n",
    "query_bigquery = trending_content \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .trigger(processingTime=TRIGGER_INTERVAL) \\\n",
    "    .foreachBatch(write_to_bigquery) \\\n",
    "    .start()\n",
    "\n",
    "print(f\"BigQuery sink started: {query_bigquery.id}\")\n",
    "print(f\"Trigger interval: {TRIGGER_INTERVAL}\")\n",
    "print(f\"Output table: {BQ_TABLE_FULL}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STREAMING QUERIES ACTIVE\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Two parallel sinks:\")\n",
    "print(\"  1. Memory table 'trending_content_memory' (for live monitoring below)\")\n",
    "print(\"  2. BigQuery table (for Looker Studio dashboard)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ae923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# CELL 4: LIVE MONITORING DISPLAY\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"REAL-TIME MONITORING DASHBOARD\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Displaying top {TOP_N_RESULTS} trending content\")\n",
    "print(f\"Update interval: {DISPLAY_INTERVAL_SECONDS} seconds\")\n",
    "print(f\"Total iterations: {DISPLAY_ITERATIONS}\")\n",
    "print(\"Press 'Interrupt Kernel' to stop\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    for iteration in range(DISPLAY_ITERATIONS):\n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(f\"UPDATE #{iteration + 1} | {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"{'=' * 80}\")\n",
    "        \n",
    "        # Query trending content from memory table\n",
    "        result_df = spark.sql(f\"\"\"\n",
    "            SELECT \n",
    "                window_start,\n",
    "                window_end,\n",
    "                title,\n",
    "                genre_primary,\n",
    "                view_count,\n",
    "                unique_viewers,\n",
    "                ROUND(avg_completion_pct, 1) as completion_pct,\n",
    "                ROUND(total_watch_time_min, 1) as watch_time_min\n",
    "            FROM trending_content_memory\n",
    "            ORDER BY view_count DESC\n",
    "            LIMIT {TOP_N_RESULTS}\n",
    "        \"\"\")\n",
    "        \n",
    "        result_df.show(TOP_N_RESULTS, truncate=False)\n",
    "        \n",
    "        # Display query status\n",
    "        print(f\"\\nQuery Status:\")\n",
    "        print(f\"  Memory sink active: {query_memory.isActive}\")\n",
    "        print(f\"  BigQuery sink active: {query_bigquery.isActive}\")\n",
    "        \n",
    "        if query_memory.recentProgress:\n",
    "            recent = query_memory.recentProgress[-1]\n",
    "            print(f\"  Recent batch: {recent.get('numInputRows', 0)} rows processed\")\n",
    "            print(f\"  Total batches: {len(query_memory.recentProgress)}\")\n",
    "        \n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(f\"Next update in {DISPLAY_INTERVAL_SECONDS} seconds... ({iteration + 1}/{DISPLAY_ITERATIONS})\")\n",
    "        print(f\"{'=' * 80}\")\n",
    "        \n",
    "        sleep(DISPLAY_INTERVAL_SECONDS)\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"STOPPING STREAMING PIPELINE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"User interrupted. Stopping queries gracefully...\")\n",
    "    \n",
    "    print(\"Stopping memory sink...\")\n",
    "    query_memory.stop()\n",
    "    print(\"Memory sink stopped\")\n",
    "    \n",
    "    print(\"Stopping BigQuery sink...\")\n",
    "    query_bigquery.stop()\n",
    "    print(\"BigQuery sink stopped\")\n",
    "    \n",
    "    print(\"\\nAll streaming queries stopped successfully\")\n",
    "    print(\"Note: Spark session is still active\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nERROR: {str(e)}\")\n",
    "    print(\"Stopping queries due to error...\")\n",
    "    try:\n",
    "        query_memory.stop()\n",
    "        query_bigquery.stop()\n",
    "    except:\n",
    "        pass\n",
    "    print(\"Queries stopped\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MONITORING COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nBigQuery table available for Looker Studio: {BQ_TABLE_FULL}\")\n",
    "print(\"You can now create visualizations in Looker Studio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ea5c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# CELL 5: CLEANUP - STOP SPARK SESSION\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CLEANUP: STOPPING SPARK SESSION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "spark.stop()\n",
    "\n",
    "print(\"Spark session stopped successfully\")\n",
    "print(\"\\nPipeline Summary:\")\n",
    "print(f\"  - Processed streaming events from: {STREAMING_DATA_PATH}\")\n",
    "print(f\"  - Output written to: {BQ_TABLE_FULL}\")\n",
    "print(f\"  - Ready for Looker Studio dashboard creation\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
