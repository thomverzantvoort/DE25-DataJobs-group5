{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc11e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import col, count, when, isnan, isnull, sum as spark_sum, avg, month, year, to_date, datediff, lit, first, max as spark_max, min as spark_min, countDistinct\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. Configuration\n",
    "# -------------------------------------------------------------------\n",
    "project_id = \"de2025-471807\"\n",
    "bq_dataset_processed = \"netflix_processed\"  # Dataset for cleaned/processed data and aggregations\n",
    "temp_bucket = \"netflix-group5-temp\"\n",
    "gcs_bucket = \"netflix_data_25\"  # GCS bucket for raw data\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. Spark session setup with GCS and BigQuery support\n",
    "# -------------------------------------------------------------------\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"BatchPipelineNetflix\")  # Changed from DataQualityCheck\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "# Use the Cloud Storage bucket for temporary BigQuery export data\n",
    "spark.conf.set('temporaryGcsBucket', temp_bucket)\n",
    "\n",
    "# Setup hadoop fs configuration for schema gs://\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "print(\"âœ“ Spark session created with GCS\")\n",
    "print(f\"   App Name: BatchPipelineNetflix\")\n",
    "print(f\"   GCS Bucket: {gcs_bucket} (reading raw data)\")\n",
    "print(f\"   Processed Dataset: {bq_dataset_processed} (writing cleaned tables + aggregations)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a575b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# 3. Load all tables from Google Cloud Storage\n",
    "# -------------------------------------------------------------------\n",
    "# Map table names to CSV files in GCS\n",
    "tables = {\n",
    "    \"users\": \"users.csv\",\n",
    "    \"movies\": \"movies.csv\",\n",
    "    \"watch_history\": \"watch_history.csv\",\n",
    "    \"recommendation_logs\": \"recommendation_logs.csv\",\n",
    "    \"reviews\": \"reviews.csv\",\n",
    "    \"search_logs\": \"search_logs.csv\"\n",
    "}\n",
    "\n",
    "dataframes = {}\n",
    "for name, csv_file in tables.items():\n",
    "    gcs_path = f\"gs://{gcs_bucket}/raw/{csv_file}\"\n",
    "    print(f\"Loading {name} from: {gcs_path}\")\n",
    "    df = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .csv(gcs_path)\n",
    "    dataframes[name] = df\n",
    "    print(f\"âœ“ Loaded {name}: {df.count()} rows, {len(df.columns)} columns\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA LOADING COMPLETE (from GCS)\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e106dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# 4. Inspect Schemas and Key Relationships\n",
    "# -------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ” SCHEMA INSPECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"\\n {name.upper()} Schema:\")\n",
    "    print(\"-\" * 80)\n",
    "    df.printSchema()\n",
    "    \n",
    "    # Check for key columns that will be used for joins\n",
    "    key_columns = []\n",
    "    if \"user_id\" in df.columns:\n",
    "        key_columns.append(\"user_id\")\n",
    "    if \"movie_id\" in df.columns:\n",
    "        key_columns.append(\"movie_id\")\n",
    "    if \"session_id\" in df.columns:\n",
    "        key_columns.append(\"session_id\")\n",
    "    \n",
    "    if key_columns:\n",
    "        print(f\"\\n  Key columns for joins: {', '.join(key_columns)}\")\n",
    "        \n",
    "        # Sample a few rows to understand data structure\n",
    "        print(f\"\\nSample data (first 3 rows):\")\n",
    "        df.select(key_columns).show(3, truncate=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Schema inspection complete\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816498d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# 5. Check missing values and duplicates\n",
    "# -------------------------------------------------------------------\n",
    "from pyspark.sql.types import DoubleType, FloatType, IntegerType, LongType, DecimalType\n",
    "\n",
    "def check_data_quality(df, name):\n",
    "    print(f\"\\nData Quality Report: {name}\")\n",
    "    total_rows = df.count()\n",
    "    print(f\"   Total rows: {total_rows}\")\n",
    "    print(f\"   Total columns: {len(df.columns)}\")\n",
    "    \n",
    "    # Missing values per column\n",
    "    missing_counts = {}\n",
    "    for col_name in df.columns:\n",
    "        col_type = dict(df.dtypes)[col_name]\n",
    "        col_expr = col(col_name)\n",
    "        \n",
    "        # Check if column is numeric (can use isnan)\n",
    "        is_numeric = col_type in ['double', 'float', 'int', 'bigint', 'decimal']\n",
    "        \n",
    "        if is_numeric:\n",
    "            # For numeric columns, check both null and nan\n",
    "            missing = df.filter(col_expr.isNull() | isnan(col_expr)).count()\n",
    "        else:\n",
    "            # For non-numeric columns, only check null\n",
    "            missing = df.filter(col_expr.isNull()).count()\n",
    "        \n",
    "        if missing > 0:\n",
    "            missing_counts[col_name] = missing\n",
    "    \n",
    "    if missing_counts:\n",
    "        print(f\"  Missing values found:\")\n",
    "        for col_name, count in missing_counts.items():\n",
    "            pct = (count / total_rows) * 100\n",
    "            print(f\"      - {col_name}: {count} ({pct:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"   âœ“ No missing values\")\n",
    "    \n",
    "    # Duplicates\n",
    "    duplicate_count = total_rows - df.dropDuplicates().count()\n",
    "    if duplicate_count > 0:\n",
    "        pct = (duplicate_count / total_rows) * 100\n",
    "        print(f\"Duplicates: {duplicate_count} rows ({pct:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"âœ“ No duplicates\")\n",
    "    \n",
    "    return missing_counts, duplicate_count\n",
    "\n",
    "# Check all dataframes\n",
    "quality_reports = {}\n",
    "for name, df in dataframes.items():\n",
    "    missing, duplicates = check_data_quality(df, name)\n",
    "    quality_reports[name] = {\"missing\": missing, \"duplicates\": duplicates}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12362fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# 6. Clean data: Remove missing values, empty columns, and duplicates\n",
    "# -------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ§¹ DATA CLEANING\")\n",
    "print(\"=\"*80)\n",
    "def clean_dataframe(df, critical_columns=None):\n",
    "    \"\"\"\n",
    "    Clean dataframe by removing:\n",
    "    - Columns that are entirely null\n",
    "    - Rows with missing values in critical columns (or all columns if not specified)\n",
    "    - Duplicate rows\n",
    "    \"\"\"\n",
    "    # Remove columns that are entirely null\n",
    "    total_rows = df.count()\n",
    "    columns_to_keep = []\n",
    "    for col_name in df.columns:\n",
    "        null_count = df.filter(col(col_name).isNull()).count()\n",
    "        if null_count < total_rows:  # Keep column if it has at least one non-null value\n",
    "            columns_to_keep.append(col_name)\n",
    "    \n",
    "    df_clean = df.select(columns_to_keep)\n",
    "    \n",
    "    # Remove rows with missing values\n",
    "    # If critical_columns specified, only check those; otherwise check all columns\n",
    "    if critical_columns:\n",
    "        # Only remove rows where critical columns are missing\n",
    "        condition = None\n",
    "        for col_name in critical_columns:\n",
    "            if col_name in df_clean.columns:\n",
    "                col_expr = col(col_name)\n",
    "                col_type = dict(df_clean.dtypes)[col_name]\n",
    "                is_numeric = col_type in ['double', 'float', 'int', 'bigint', 'decimal']\n",
    "                \n",
    "                if is_numeric:\n",
    "                    col_condition = col_expr.isNull() | isnan(col_expr)\n",
    "                else:\n",
    "                    col_condition = col_expr.isNull()\n",
    "                \n",
    "                if condition is None:\n",
    "                    condition = col_condition\n",
    "                else:\n",
    "                    condition = condition | col_condition\n",
    "        \n",
    "        if condition is not None:\n",
    "            df_clean = df_clean.filter(~condition)\n",
    "    else:\n",
    "        # Remove rows with any missing values (original behavior)\n",
    "        df_clean = df_clean.dropna()\n",
    "    \n",
    "    # Remove duplicate rows\n",
    "    df_clean = df_clean.dropDuplicates()\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Define critical columns for each table (columns that must not be null)\n",
    "critical_columns_map = {\n",
    "    \"users\": [\"user_id\", \"email\"],  # User must have ID and email\n",
    "    \"movies\": [\"movie_id\", \"title\"],  # Movie must have ID and title\n",
    "    \"watch_history\": [\"session_id\", \"user_id\", \"movie_id\"],  # Watch session must have these\n",
    "    \"recommendation_logs\": [\"user_id\", \"movie_id\"],  # Recommendation must have user and movie\n",
    "    \"reviews\": [\"user_id\", \"movie_id\"],  # Review must have user and movie\n",
    "    \"search_logs\": [\"user_id\"]  # Search must have user\n",
    "}\n",
    "\n",
    "cleaned_dataframes = {}\n",
    "for name, df in dataframes.items():\n",
    "    original_count = df.count()\n",
    "    original_cols = len(df.columns)\n",
    "    critical_cols = critical_columns_map.get(name, None)\n",
    "    df_clean = clean_dataframe(df, critical_columns=critical_cols)\n",
    "    cleaned_count = df_clean.count()\n",
    "    cleaned_cols = len(df_clean.columns)\n",
    "    cleaned_dataframes[name] = df_clean\n",
    "    \n",
    "    removed_rows = original_count - cleaned_count\n",
    "    removed_cols = original_cols - cleaned_cols\n",
    "    print(f\"{name}: {original_count} â†’ {cleaned_count} rows, {original_cols} â†’ {cleaned_cols} cols (removed {removed_rows} rows, {removed_cols} cols)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bac2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# 7. Save cleaned data to BigQuery\n",
    "# -------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING CLEANED DATA TO BIGQUERY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nWriting cleaned data to BigQuery...\")\n",
    "\n",
    "# Map table names for BigQuery (use same names as raw, or add suffix)\n",
    "table_name_map = {\n",
    "    \"users\": \"Users\",\n",
    "    \"movies\": \"Movies\",\n",
    "    \"watch_history\": \"WatchHistory\",\n",
    "    \"recommendation_logs\": \"RecommendationLogs\",\n",
    "    \"reviews\": \"Reviews\",\n",
    "    \"search_logs\": \"SearchLogs\"\n",
    "}\n",
    "\n",
    "for name, df_clean in cleaned_dataframes.items():\n",
    "    table_name = table_name_map.get(name, name.capitalize())\n",
    "    bq_table = f\"{project_id}.{bq_dataset_processed}.{table_name}\"\n",
    "    \n",
    "    print(f\"   Writing {name} to {bq_table}...\")\n",
    "    df_clean.write.format('bigquery') \\\n",
    "        .option('table', bq_table) \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save()\n",
    "    print(f\"   {name} written successfully ({df_clean.count()} rows)\")\n",
    "\n",
    "print(f\"\\n âœ“ All cleaned data written to BigQuery dataset: {bq_dataset_processed}\")\n",
    "print(\"\\nData quality check and cleaning completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b5bb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# 8. Join Tables (Star Schema)\n",
    "# -------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ”— JOINING TABLES (STAR SCHEMA)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nBuilding star schema: watch_history as fact table, others as dimensions...\")\n",
    "\n",
    "# Start with the fact table (watch_history)\n",
    "fact_table = cleaned_dataframes[\"watch_history\"]\n",
    "print(f\"\\n âœ“ Fact table (watch_history): {fact_table.count()} rows\")\n",
    "\n",
    "# Join with dimension tables\n",
    "# 1. Join with users (dimension)\n",
    "joined_df = fact_table.join(\n",
    "    cleaned_dataframes[\"users\"],\n",
    "    on=\"user_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "print(f\"âœ“ After joining with users: {joined_df.count()} rows\")\n",
    "\n",
    "# 2. Join with movies (dimension)\n",
    "joined_df = joined_df.join(\n",
    "    cleaned_dataframes[\"movies\"],\n",
    "    on=\"movie_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "print(f\"âœ“ After joining with movies: {joined_df.count()} rows\")\n",
    "\n",
    "# 3. Optionally join with reviews (for rating information)\n",
    "# Use left join to keep all watch history even if no review exists\n",
    "# Rename reviews.rating to user_rating to avoid conflict with movies.rating (content rating)\n",
    "# Aggregate reviews: if user has multiple reviews for same movie, take average rating\n",
    "reviews_for_join = cleaned_dataframes[\"reviews\"].groupBy(\n",
    "    \"user_id\", \n",
    "    \"movie_id\"\n",
    ").agg(\n",
    "    avg(\"rating\").alias(\"user_rating\")\n",
    ")\n",
    "\n",
    "# Drop user_rating if it already exists (to avoid ambiguity)\n",
    "if \"user_rating\" in joined_df.columns:\n",
    "    joined_df = joined_df.drop(\"user_rating\")\n",
    "\n",
    "joined_df = joined_df.join(\n",
    "    reviews_for_join,\n",
    "    on=[\"user_id\", \"movie_id\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "print(f\"âœ“ After joining with reviews: {joined_df.count()} rows\")\n",
    "\n",
    "# Show sample of joined data\n",
    "print(\"\\n Sample of joined data (first 5 rows):\")\n",
    "print(\"-\" * 80)\n",
    "joined_df.select(\n",
    "    \"session_id\", \"user_id\", \"movie_id\", \"watch_date\",\n",
    "    \"country\", \"subscription_plan\", \"title\", \"genre_primary\",\n",
    "    \"watch_duration_minutes\", \"action\"\n",
    ").show(5, truncate=False)\n",
    "\n",
    "print(\"\\n âœ“ Star schema join complete!\")\n",
    "print(f\"   Final joined dataset: {joined_df.count()} rows, {len(joined_df.columns)} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56640f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# 9. Transform Data (Parse Timestamps, Prepare for Aggregations)\n",
    "# -------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ”„ DATA TRANSFORMATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "# Parse watch_date to timestamp if it's not already\n",
    "# Check the current type\n",
    "print(\"\\nðŸ“… Checking date column types...\")\n",
    "print(f\"   watch_date type: {dict(joined_df.dtypes).get('watch_date', 'N/A')}\")\n",
    "\n",
    "# Convert watch_date to timestamp if it's a string\n",
    "if 'watch_date' in joined_df.columns:\n",
    "    # Try to parse as timestamp\n",
    "    joined_df = joined_df.withColumn(\n",
    "        \"watch_date_parsed\",\n",
    "        to_timestamp(col(\"watch_date\"), \"yyyy-MM-dd HH:mm:ss\")\n",
    "    )\n",
    "    \n",
    "    # Extract year and month for monthly aggregations\n",
    "    joined_df = joined_df.withColumn(\"watch_year\", year(col(\"watch_date_parsed\")))\n",
    "    joined_df = joined_df.withColumn(\"watch_month\", month(col(\"watch_date_parsed\")))\n",
    "    \n",
    "    print(\"âœ“ Parsed watch_date and extracted year/month\")\n",
    "\n",
    "# Handle missing watch_duration_minutes (fill with 0 or median)\n",
    "# For now, we'll filter out nulls in aggregations, but we could also fill\n",
    "print(f\"\\n Data quality after transformation:\")\n",
    "print(f\"   Total rows: {joined_df.count()}\")\n",
    "print(f\"   Rows with watch_duration_minutes: {joined_df.filter(col('watch_duration_minutes').isNotNull()).count()}\")\n",
    "print(f\"   Rows with watch_date_parsed: {joined_df.filter(col('watch_date_parsed').isNotNull()).count()}\")\n",
    "\n",
    "print(\"\\nâœ“ Data transformation complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dedcef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# 10. Content Performance Aggregations\n",
    "# -------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“ˆ CONTENT PERFORMANCE AGGREGATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Filter out null values for aggregations\n",
    "df_for_agg = joined_df.filter(\n",
    "    col(\"watch_date_parsed\").isNotNull() &\n",
    "    col(\"watch_duration_minutes\").isNotNull()\n",
    ")\n",
    "\n",
    "# 1. Average rating per genre (monthly)\n",
    "print(\"\\n 1. Computing average rating per genre (monthly)...\")\n",
    "content_performance = df_for_agg.filter(col(\"user_rating\").isNotNull()).groupBy(\n",
    "    \"watch_year\",\n",
    "    \"watch_month\",\n",
    "    \"genre_primary\"\n",
    ").agg(\n",
    "    avg(\"user_rating\").alias(\"avg_rating\"),\n",
    "    count(\"*\").alias(\"total_views\"),\n",
    "    spark_sum(\"watch_duration_minutes\").alias(\"total_watch_time_minutes\"),\n",
    "    countDistinct(\"movie_id\").alias(\"unique_movies\"),\n",
    "    countDistinct(\"user_id\").alias(\"unique_users\")\n",
    ").orderBy(\"watch_year\", \"watch_month\", \"genre_primary\")\n",
    "\n",
    "print(\"âœ“ Content performance aggregation complete\")\n",
    "print(f\"   Rows in content_performance: {content_performance.count()}\")\n",
    "print(\"\\n Sample content performance data:\")\n",
    "content_performance.show(10, truncate=False)\n",
    "\n",
    "# 2. Genre performance over time (overall, not just monthly)\n",
    "print(\"\\n 2. Computing overall genre performance...\")\n",
    "genre_performance = df_for_agg.groupBy(\"genre_primary\").agg(\n",
    "    count(\"*\").alias(\"total_views\"),\n",
    "    spark_sum(\"watch_duration_minutes\").alias(\"total_watch_time_minutes\"),\n",
    "    avg(\"watch_duration_minutes\").alias(\"avg_watch_duration\"),\n",
    "    countDistinct(\"movie_id\").alias(\"unique_movies\"),\n",
    "    countDistinct(\"user_id\").alias(\"unique_users\"),\n",
    "    avg(\"user_rating\").alias(\"avg_rating\")\n",
    ").orderBy(spark_sum(\"watch_duration_minutes\").desc())\n",
    "\n",
    "print(\"âœ“ Genre performance aggregation complete\")\n",
    "print(f\"   Rows in genre_performance: {genre_performance.count()}\")\n",
    "print(\"\\n Top genres by watch time:\")\n",
    "genre_performance.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207f6d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# 11. User Engagement Aggregations\n",
    "# -------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ‘¥ USER ENGAGEMENT AGGREGATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Monthly Engagement: Total watch time per country and plan\n",
    "print(\"\\n 1. Computing monthly engagement (watch time per country and plan)...\")\n",
    "monthly_engagement = df_for_agg.groupBy(\n",
    "    \"watch_year\",\n",
    "    \"watch_month\",\n",
    "    \"country\",\n",
    "    \"subscription_plan\"\n",
    ").agg(\n",
    "    spark_sum(\"watch_duration_minutes\").alias(\"total_watch_time_minutes\"),\n",
    "    countDistinct(\"user_id\").alias(\"monthly_active_users\"),\n",
    "    count(\"*\").alias(\"total_sessions\"),\n",
    "    avg(\"watch_duration_minutes\").alias(\"avg_session_duration\"),\n",
    "    countDistinct(\"movie_id\").alias(\"unique_content_viewed\")\n",
    ").orderBy(\"watch_year\", \"watch_month\", \"country\", \"subscription_plan\")\n",
    "\n",
    "print(\"âœ“ Monthly engagement aggregation complete\")\n",
    "print(f\"   Rows in monthly_engagement: {monthly_engagement.count()}\")\n",
    "print(\"\\n Sample monthly engagement data:\")\n",
    "monthly_engagement.show(10, truncate=False)\n",
    "\n",
    "# 2. Monthly Active Users (MAU) - overall\n",
    "print(\"\\n 2. Computing Monthly Active Users (MAU)...\")\n",
    "mau = df_for_agg.groupBy(\n",
    "    \"watch_year\",\n",
    "    \"watch_month\"\n",
    ").agg(\n",
    "    countDistinct(\"user_id\").alias(\"monthly_active_users\"),\n",
    "    countDistinct(\"country\").alias(\"countries\"),\n",
    "    spark_sum(\"watch_duration_minutes\").alias(\"total_watch_time_minutes\")\n",
    ").orderBy(\"watch_year\", \"watch_month\")\n",
    "\n",
    "print(\"âœ“ MAU aggregation complete\")\n",
    "print(f\"   Rows in MAU: {mau.count()}\")\n",
    "print(\"\\n Monthly Active Users:\")\n",
    "mau.show(20, truncate=False)\n",
    "\n",
    "# 3. Cohort Retention Analysis\n",
    "print(\"\\ 3. Computing cohort retention...\")\n",
    "# Get user's first watch date (cohort)\n",
    "user_cohorts = df_for_agg.groupBy(\"user_id\").agg(\n",
    "    spark_min(\"watch_date_parsed\").alias(\"first_watch_date\")\n",
    ").withColumn(\"cohort_year\", year(col(\"first_watch_date\"))) \\\n",
    " .withColumn(\"cohort_month\", month(col(\"first_watch_date\")))\n",
    "\n",
    "# Join back to get all user activity\n",
    "user_activity = df_for_agg.join(\n",
    "    user_cohorts.select(\"user_id\", \"cohort_year\", \"cohort_month\"),\n",
    "    on=\"user_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Calculate retention: users active in each month relative to their cohort\n",
    "cohort_retention = user_activity.groupBy(\n",
    "    \"cohort_year\",\n",
    "    \"cohort_month\",\n",
    "    \"watch_year\",\n",
    "    \"watch_month\"\n",
    ").agg(\n",
    "    countDistinct(\"user_id\").alias(\"active_users\")\n",
    ").withColumn(\n",
    "    \"months_since_cohort\",\n",
    "    (col(\"watch_year\") - col(\"cohort_year\")) * 12 + (col(\"watch_month\") - col(\"cohort_month\"))\n",
    ").orderBy(\"cohort_year\", \"cohort_month\", \"watch_year\", \"watch_month\")\n",
    "\n",
    "print(\"âœ“ Cohort retention aggregation complete\")\n",
    "print(f\"   Rows in cohort_retention: {cohort_retention.count()}\")\n",
    "print(\"\\n Sample cohort retention data:\")\n",
    "cohort_retention.show(20, truncate=False)\n",
    "\n",
    "print(\"\\n âœ“ All user engagement aggregations complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f91c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# 12. Write Aggregated Data to BigQuery\n",
    "# -------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"WRITING AGGREGATED DATA TO BIGQUERY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Write monthly_engagement table (as required by assignment)\n",
    "print(\"\\n 1. Writing monthly_engagement table...\")\n",
    "monthly_engagement.write.format('bigquery') \\\n",
    "    .option('table', f\"{project_id}.{bq_dataset_processed}.monthly_engagement\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "print(f\"   âœ“ monthly_engagement written to {bq_dataset_processed}.monthly_engagement\")\n",
    "print(f\"   Rows: {monthly_engagement.count()}\")\n",
    "\n",
    "# Write cohort_retention table (as required by assignment)\n",
    "print(\"\\n 2. Writing cohort_retention table...\")\n",
    "cohort_retention.write.format('bigquery') \\\n",
    "    .option('table', f\"{project_id}.{bq_dataset_processed}.cohort_retention\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "print(f\"   âœ“ cohort_retention written to {bq_dataset_processed}.cohort_retention\")\n",
    "print(f\"   Rows: {cohort_retention.count()}\")\n",
    "\n",
    "# Write additional aggregated tables for dashboard\n",
    "print(\"\\n 3. Writing additional aggregated tables...\")\n",
    "\n",
    "# Content performance\n",
    "content_performance.write.format('bigquery') \\\n",
    "    .option('table', f\"{project_id}.{bq_dataset_processed}.content_performance\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "print(f\"   âœ“ content_performance written to {bq_dataset_processed}.content_performance\")\n",
    "\n",
    "# Genre performance\n",
    "genre_performance.write.format('bigquery') \\\n",
    "    .option('table', f\"{project_id}.{bq_dataset_processed}.genre_performance\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "print(f\"   âœ“ genre_performance written to {bq_dataset_processed}.genre_performance\")\n",
    "\n",
    "# MAU\n",
    "mau.write.format('bigquery') \\\n",
    "    .option('table', f\"{project_id}.{bq_dataset_processed}.monthly_active_users\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "print(f\"   âœ“ monthly_active_users written to {bq_dataset_processed}.monthly_active_users\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BATCH PIPELINE COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n âœ“All data written to BigQuery dataset: {bq_dataset_processed}\")\n",
    "print(\"\\n Summary of outputs:\")\n",
    "print(f\"   - Cleaned tables:\")\n",
    "print(f\"     â€¢ Users, Movies, WatchHistory, RecommendationLogs, Reviews, SearchLogs\")\n",
    "print(f\"   - Aggregated tables:\")\n",
    "print(f\"     â€¢ monthly_engagement (required)\")\n",
    "print(f\"     â€¢ cohort_retention (required)\")\n",
    "print(f\"     â€¢ content_performance (optional)\")\n",
    "print(f\"     â€¢ genre_performance (optional)\")\n",
    "print(f\"     â€¢ monthly_active_users (optional)\")\n",
    "print(\"\\nReady for Looker Studio dashboard creation!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d539e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark context\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
