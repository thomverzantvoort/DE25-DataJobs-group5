{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299a5692",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPySparkRuntimeError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m col, count, when, isnan, isnull\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# -------------------------------------------------------------------\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# 1. Spark session setup\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# -------------------------------------------------------------------\u001b[39;00m\n\u001b[32m      7\u001b[39m spark = (\n\u001b[32m      8\u001b[39m     \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDataQualityCheck\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark://spark-master:7077\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.driver.memory\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m2g\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.executor.memory\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m2g\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m )\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Data path\u001b[39;00m\n\u001b[32m     17\u001b[39m data_path = \u001b[33m\"\u001b[39m\u001b[33m/home/jovyan/data/\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rderi\\OneDrive\\Bureaublad\\Jads 2025-2027\\Data Engineering\\Data Engineering Assignment\\DE-Data-Architecture-Pipelines-Group5\\.venv\\Lib\\site-packages\\pyspark\\sql\\session.py:556\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    554\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    555\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    559\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rderi\\OneDrive\\Bureaublad\\Jads 2025-2027\\Data Engineering\\Data Engineering Assignment\\DE-Data-Architecture-Pipelines-Group5\\.venv\\Lib\\site-packages\\pyspark\\core\\context.py:523\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rderi\\OneDrive\\Bureaublad\\Jads 2025-2027\\Data Engineering\\Data Engineering Assignment\\DE-Data-Architecture-Pipelines-Group5\\.venv\\Lib\\site-packages\\pyspark\\core\\context.py:205\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway.gateway_parameters.auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    201\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not allowed as it is a security risk.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    203\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28mself\u001b[39m._do_init(\n\u001b[32m    208\u001b[39m         master,\n\u001b[32m    209\u001b[39m         appName,\n\u001b[32m   (...)\u001b[39m\u001b[32m    219\u001b[39m         memory_profiler_cls,\n\u001b[32m    220\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rderi\\OneDrive\\Bureaublad\\Jads 2025-2027\\Data Engineering\\Data Engineering Assignment\\DE-Data-Architecture-Pipelines-Group5\\.venv\\Lib\\site-packages\\pyspark\\core\\context.py:444\u001b[39m, in \u001b[36mSparkContext._ensure_initialized\u001b[39m\u001b[34m(cls, instance, gateway, conf)\u001b[39m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    443\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext._gateway:\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m         SparkContext._gateway = gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    445\u001b[39m         SparkContext._jvm = SparkContext._gateway.jvm\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rderi\\OneDrive\\Bureaublad\\Jads 2025-2027\\Data Engineering\\Data Engineering Assignment\\DE-Data-Architecture-Pipelines-Group5\\.venv\\Lib\\site-packages\\pyspark\\java_gateway.py:111\u001b[39m, in \u001b[36mlaunch_gateway\u001b[39m\u001b[34m(conf, popen_kwargs)\u001b[39m\n\u001b[32m    108\u001b[39m     time.sleep(\u001b[32m0.1\u001b[39m)\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[32m    112\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mJAVA_GATEWAY_EXITED\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    113\u001b[39m         messageParameters={},\n\u001b[32m    114\u001b[39m     )\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[32m    117\u001b[39m     gateway_port = read_int(info)\n",
      "\u001b[31mPySparkRuntimeError\u001b[39m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import col, count, when, isnan, isnull\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1. Configuration\n",
    "# -------------------------------------------------------------------\n",
    "project_id = \"de2025-471807\"\n",
    "bq_dataset_raw = \"netflix\"  # Dataset for raw data\n",
    "bq_dataset_processed = \"netflix_processed\"  # Dataset for cleaned/processed data\n",
    "temp_bucket = \"netflix-group5-temp\"\n",
    "processed_path = \"/home/jovyan/data/processed/\"  # Optional: also save to local CSV\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2. Spark session setup with BigQuery\n",
    "# -------------------------------------------------------------------\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"DataQualityCheck\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "# Use the Cloud Storage bucket for temporary BigQuery export data\n",
    "spark.conf.set('temporaryGcsBucket', temp_bucket)\n",
    "\n",
    "# Setup hadoop fs configuration for schema gs://\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "print(\"‚úÖ Spark session created with BigQuery support\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f3e48d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m dataframes = {}\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, filename \u001b[38;5;129;01min\u001b[39;00m files.items():\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     df = \u001b[43mspark\u001b[49m.read.option(\u001b[33m\"\u001b[39m\u001b[33mheader\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m).csv(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m     dataframes[name] = df\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Loaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.count()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df.columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m columns\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# 3. Load all tables from BigQuery\n",
    "# -------------------------------------------------------------------\n",
    "tables = {\n",
    "    \"users\": \"Users\",\n",
    "    \"movies\": \"Movies\",\n",
    "    \"watch_history\": \"WatchHistory\",\n",
    "    \"recommendation_logs\": \"RecommendationLogs\",\n",
    "    \"reviews\": \"Reviews\",\n",
    "    \"search_logs\": \"SearchLogs\"\n",
    "}\n",
    "\n",
    "dataframes = {}\n",
    "for name, table_name in tables.items():\n",
    "    df = spark.read.format(\"bigquery\").load(f\"{project_id}.{bq_dataset_raw}.{table_name}\")\n",
    "    dataframes[name] = df\n",
    "    print(f\"‚úÖ Loaded {name}: {df.count()} rows, {len(df.columns)} columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816498d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# 4. Check missing values and duplicates\n",
    "# -------------------------------------------------------------------\n",
    "from pyspark.sql.types import DoubleType, FloatType, IntegerType, LongType, DecimalType\n",
    "\n",
    "def check_data_quality(df, name):\n",
    "    print(f\"\\nüìä Data Quality Report: {name}\")\n",
    "    total_rows = df.count()\n",
    "    print(f\"   Total rows: {total_rows}\")\n",
    "    print(f\"   Total columns: {len(df.columns)}\")\n",
    "    \n",
    "    # Missing values per column\n",
    "    missing_counts = {}\n",
    "    for col_name in df.columns:\n",
    "        col_type = dict(df.dtypes)[col_name]\n",
    "        col_expr = col(col_name)\n",
    "        \n",
    "        # Check if column is numeric (can use isnan)\n",
    "        is_numeric = col_type in ['double', 'float', 'int', 'bigint', 'decimal']\n",
    "        \n",
    "        if is_numeric:\n",
    "            # For numeric columns, check both null and nan\n",
    "            missing = df.filter(col_expr.isNull() | isnan(col_expr)).count()\n",
    "        else:\n",
    "            # For non-numeric columns, only check null\n",
    "            missing = df.filter(col_expr.isNull()).count()\n",
    "        \n",
    "        if missing > 0:\n",
    "            missing_counts[col_name] = missing\n",
    "    \n",
    "    if missing_counts:\n",
    "        print(f\"   ‚ö†Ô∏è  Missing values found:\")\n",
    "        for col_name, count in missing_counts.items():\n",
    "            pct = (count / total_rows) * 100\n",
    "            print(f\"      - {col_name}: {count} ({pct:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ No missing values\")\n",
    "    \n",
    "    # Duplicates\n",
    "    duplicate_count = total_rows - df.dropDuplicates().count()\n",
    "    if duplicate_count > 0:\n",
    "        pct = (duplicate_count / total_rows) * 100\n",
    "        print(f\"   ‚ö†Ô∏è  Duplicates: {duplicate_count} rows ({pct:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"   ‚úÖ No duplicates\")\n",
    "    \n",
    "    return missing_counts, duplicate_count\n",
    "\n",
    "# Check all dataframes\n",
    "quality_reports = {}\n",
    "for name, df in dataframes.items():\n",
    "    missing, duplicates = check_data_quality(df, name)\n",
    "    quality_reports[name] = {\"missing\": missing, \"duplicates\": duplicates}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12362fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# 5. Clean data: Remove missing values, empty columns, and duplicates\n",
    "# -------------------------------------------------------------------\n",
    "def clean_dataframe(df, critical_columns=None):\n",
    "    \"\"\"\n",
    "    Clean dataframe by removing:\n",
    "    - Columns that are entirely null\n",
    "    - Rows with missing values in critical columns (or all columns if not specified)\n",
    "    - Duplicate rows\n",
    "    \"\"\"\n",
    "    # Remove columns that are entirely null\n",
    "    total_rows = df.count()\n",
    "    columns_to_keep = []\n",
    "    for col_name in df.columns:\n",
    "        null_count = df.filter(col(col_name).isNull()).count()\n",
    "        if null_count < total_rows:  # Keep column if it has at least one non-null value\n",
    "            columns_to_keep.append(col_name)\n",
    "    \n",
    "    df_clean = df.select(columns_to_keep)\n",
    "    \n",
    "    # Remove rows with missing values\n",
    "    # If critical_columns specified, only check those; otherwise check all columns\n",
    "    if critical_columns:\n",
    "        # Only remove rows where critical columns are missing\n",
    "        condition = None\n",
    "        for col_name in critical_columns:\n",
    "            if col_name in df_clean.columns:\n",
    "                col_expr = col(col_name)\n",
    "                col_type = dict(df_clean.dtypes)[col_name]\n",
    "                is_numeric = col_type in ['double', 'float', 'int', 'bigint', 'decimal']\n",
    "                \n",
    "                if is_numeric:\n",
    "                    col_condition = col_expr.isNull() | isnan(col_expr)\n",
    "                else:\n",
    "                    col_condition = col_expr.isNull()\n",
    "                \n",
    "                if condition is None:\n",
    "                    condition = col_condition\n",
    "                else:\n",
    "                    condition = condition | col_condition\n",
    "        \n",
    "        if condition is not None:\n",
    "            df_clean = df_clean.filter(~condition)\n",
    "    else:\n",
    "        # Remove rows with any missing values (original behavior)\n",
    "        df_clean = df_clean.dropna()\n",
    "    \n",
    "    # Remove duplicate rows\n",
    "    df_clean = df_clean.dropDuplicates()\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Define critical columns for each table (columns that must not be null)\n",
    "critical_columns_map = {\n",
    "    \"users\": [\"user_id\", \"email\"],  # User must have ID and email\n",
    "    \"movies\": [\"movie_id\", \"title\"],  # Movie must have ID and title\n",
    "    \"watch_history\": [\"session_id\", \"user_id\", \"movie_id\"],  # Watch session must have these\n",
    "    \"recommendation_logs\": [\"user_id\", \"movie_id\"],  # Recommendation must have user and movie\n",
    "    \"reviews\": [\"user_id\", \"movie_id\"],  # Review must have user and movie\n",
    "    \"search_logs\": [\"user_id\"]  # Search must have user\n",
    "}\n",
    "\n",
    "cleaned_dataframes = {}\n",
    "for name, df in dataframes.items():\n",
    "    original_count = df.count()\n",
    "    original_cols = len(df.columns)\n",
    "    critical_cols = critical_columns_map.get(name, None)\n",
    "    df_clean = clean_dataframe(df, critical_columns=critical_cols)\n",
    "    cleaned_count = df_clean.count()\n",
    "    cleaned_cols = len(df_clean.columns)\n",
    "    cleaned_dataframes[name] = df_clean\n",
    "    \n",
    "    removed_rows = original_count - cleaned_count\n",
    "    removed_cols = original_cols - cleaned_cols\n",
    "    print(f\"‚úÖ {name}: {original_count} ‚Üí {cleaned_count} rows, {original_cols} ‚Üí {cleaned_cols} cols (removed {removed_rows} rows, {removed_cols} cols)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bac2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# 6. Save cleaned data to BigQuery\n",
    "# -------------------------------------------------------------------\n",
    "print(\"\\nüì§ Writing cleaned data to BigQuery...\")\n",
    "\n",
    "# Map table names for BigQuery (use same names as raw, or add suffix)\n",
    "table_name_map = {\n",
    "    \"users\": \"Users\",\n",
    "    \"movies\": \"Movies\",\n",
    "    \"watch_history\": \"WatchHistory\",\n",
    "    \"recommendation_logs\": \"RecommendationLogs\",\n",
    "    \"reviews\": \"Reviews\",\n",
    "    \"search_logs\": \"SearchLogs\"\n",
    "}\n",
    "\n",
    "for name, df_clean in cleaned_dataframes.items():\n",
    "    table_name = table_name_map.get(name, name.capitalize())\n",
    "    bq_table = f\"{project_id}.{bq_dataset_processed}.{table_name}\"\n",
    "    \n",
    "    print(f\"   Writing {name} to {bq_table}...\")\n",
    "    df_clean.write.format('bigquery') \\\n",
    "        .option('table', bq_table) \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save()\n",
    "    print(f\"   ‚úÖ {name} written successfully ({df_clean.count()} rows)\")\n",
    "\n",
    "print(f\"\\n‚úÖ All cleaned data written to BigQuery dataset: {bq_dataset_processed}\")\n",
    "print(\"\\nüéâ Data quality check and cleaning completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
