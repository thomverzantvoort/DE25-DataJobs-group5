Netflix User Intelligence – Data Engineering Project

Overview
This project is developed for the JADS Data Engineering course (Assignment 2). The goal is to design and implement a Big Data Architecture and two Spark data processing pipelines that generate both real-time and historical insights into user behavior on a streaming platform. The project is based on the Netflix 2025 User Behavior Dataset from Kaggle, which contains user activity, subscription, and content data.

The system is implemented on Google Cloud Platform (GCP) using Apache Spark, Kafka, BigQuery, and Looker Studio, and deployed using Docker Compose as demonstrated in Lab 6 – Implementing Big Data Architectures with Containers.

---

Dataset Description
The project uses the following datasets from the Netflix 2025 User Behavior collection on Kaggle:

1. users.csv contains user account and demographic attributes such as user identifier, country, device type, and plan.
2. movies.csv contains catalog metadata such as show or movie identifier, title, genre, release year, and duration.
3. watch_history.csv contains granular viewing events per user with timestamps, play duration, completion percentage, and event type such as play, pause, or stop
4. reviews.csv contains user generated ratings or textual reviews per movie.
5. search_logs.csv contains user search events with timestamp, query text, and optional clicked item.
6. recommendation_logs.csv contains recommendation delivery or click events showing which items were recommended and whether a user interacted.

The dataset follows a star schema where watch_history.csv acts as the fact table and all other files act as dimension tables. This structure enables joining and aggregating data efficiently during processing.

---

Step-by-Step Implementation Plan

Step 1 – Implementing the Big Data Architecture with Containers
Create a modular and reproducible environment using Docker Compose.
The architecture will include:

* Zookeeper and Kafka for streaming event ingestion
* Spark master and worker nodes for distributed batch and streaming jobs
* Google Cloud Storage (GCS) as raw and intermediate storage
* BigQuery as the serving and analytical warehouse
* Looker Studio for visualization

Step 2 – Preparing and Uploading Data to GCP
Download the Netflix dataset from Kaggle and upload each CSV file to a Cloud Storage bucket under a /raw folder.
Example folder structure:

Inspect the schema and confirm the consistency of keys such as user_id and show_id. These will be used for joins and aggregations.

Step 3 – Developing the Batch Data Pipeline
Implement a PySpark batch job to process and aggregate historical data:

1. Load all raw CSVs from Cloud Storage.
2. Join tables based on their key relationships (user_id, show_id).
3. Clean and transform data (filter invalid rows, parse timestamps).
4. Compute monthly aggregates such as:

   * Total watch time per country and plan
   * Average rating per genre
   * Monthly active users and retention rate
5. Write results to BigQuery tables:

   * monthly_engagement
   * cohort_retention
6. Automate this batch process using a Crontab schedule.

This pipeline provides insights into user engagement trends and subscription retention.

Step 4 – Implementing the Streaming Data Pipeline
Develop a Spark Structured Streaming job that reads simulated real-time events from Kafka:

1. Create a Kafka producer that replays events from watch_history.csv as JSON messages.
2. Define a Spark Structured Streaming consumer that reads from the Kafka topic.
3. Parse and validate messages, apply watermarking for late data, and perform time-windowed aggregations.
4. Compute real-time metrics such as:

   * Active users per minute
   * Event type distribution (plays, pauses, skips)
   * Average session duration
5. Continuously write aggregated data to BigQuery in append mode.

This pipeline supports real-time dashboards and monitoring.

Step 5 – Designing Dashboards in Looker Studio
Connect the BigQuery tables generated by both pipelines to Looker Studio to visualize key metrics.

Dashboard 1: Real-Time Engagement

* Active users per minute
* Event type distribution
* Top genres and countries
* Average session duration

Dashboard 2: Historical Trends

* Monthly active users (MAU) per plan and country
* Retention and churn analysis
* Skip rate and watch duration trends
* Genre performance over time

